{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu6IqvrhJsOX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ge4YmoJZhy",
        "outputId": "bb4cbafc-65ae-4ce1-b230-778ef7e41f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount the content of drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YnIESTSJn7F"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow --quiet\n",
        "!pip install torchmetrics --quiet\n",
        "\n",
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n"
      ],
      "metadata": {
        "id": "9WwcC3GTovRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A5ZzNyrJrSf"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok --quiet\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "path_uri = 'PATH_TO_ARTIFACTORY'\n",
        "mlflow.set_registry_uri(path_uri)\n",
        "mlflow.set_tracking_uri(path_uri)\n",
        "print(\"Tracking Location: {}\".format(mlflow.get_tracking_uri()))\n",
        "print(\"Registry Location: {}\".format(mlflow.get_registry_uri()))\n",
        "\n",
        "# Run once if you run it for the first time\n",
        "# experiment_id = mlflow.create_experiment(name=\"Experiments\", artifact_location=path_uri)\n",
        "experiment = mlflow.get_experiment_by_name('Experiments')\n",
        "print(\"Name: {}\".format(experiment.name))\n",
        "print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
        "print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
        "print(\"Tags: {}\".format(experiment.tags))\n",
        "print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
        "\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(f\"mlflow ui --backend-store-uri {PATH_TO_ARTIFACTORY} --default-artifact-root {PATH_TO_ARTIFACTORY} --port 5000 &\") # run tracking UI in the background\n",
        "\n",
        "\n",
        "# create remote tunnel using ngrok.com to allow local port access\n",
        "# borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"YOUR_TOKEN\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBOYa_w9OxwH"
      },
      "outputs": [],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "print(\"Tracking Location: {}\".format(mlflow.get_tracking_uri()))\n",
        "print(\"Registry Location: {}\".format(mlflow.get_registry_uri()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INFkAB4SNZC7"
      },
      "outputs": [],
      "source": [
        "# Insert the directory\n",
        "import sys\n",
        "PATH_TO_VOC_TOOLS = 'path'\n",
        "sys.path.insert(0,PATH_TO_VOC_TOOLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slnz_ujjNZRN",
        "outputId": "5076a390-4d9f-465d-f31f-05f851c3b88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.7/dist-packages (2021.11.2)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tifffile) (1.21.6)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Import & install all tools \n",
        "!pip install segmentation_models_pytorch --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install tifffile\n",
        "!pip install cython  --quiet\n",
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git  --quiet\n",
        "!pip uninstall --yes opencv-python-headless==4.5.5.62  --quiet\n",
        "!pip install opencv-python-headless==4.5.2.52  --quiet\n",
        "!pip install grad-cam --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnwa23gLNviy"
      },
      "outputs": [],
      "source": [
        "# Import custom function\n",
        "from voc_tools import (\n",
        "    return_files_in_directory,\n",
        "    decode_segmap,\n",
        "    get_classes_from_mask,\n",
        "    rgb_to_mask,\n",
        "    visualize,\n",
        "    make_prediction_on_image,\n",
        "    pull_images_to_directories,\n",
        "    get_classes_from_mask,\n",
        "    return_batch_information,\n",
        "    flatten\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1mVFkTOE_V"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import compute_unary, unary_from_softmax\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.filters import sobel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "import copy\n",
        "from tifffile import imread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spg6F5McOeaV",
        "outputId": "d0749fcc-9e72-4ac5-88d2-963f48fad5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 24 17:13:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "graDYsMzOkP8"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvVcSSJkOhiq"
      },
      "outputs": [],
      "source": [
        "# Some training settings\n",
        "DATA_DIR = 'PATH_TO_COLONOSCOPY_DATA_DIRECTORY'\n",
        "BEST_MODEL_DIR = 'WHERE_TO_SAVE_BEST_MODEL_WEIGHTS'\n",
        "CHECKPOINT_MODEL_DIR = 'WHERE_TO_SAVE_CHECKPOINT_MODEL_WEIGHTS'\n",
        "EXPORT_CSV_DIR = \"WHERE_TO_SAVE_TRAINING_METRICS\"\n",
        "EVAL_ON_MASKS = True\n",
        "TRAINING_INPUT = \"Boxes\"\n",
        "STATE = '_'.join([\"Benchmarking\", TRAINING_INPUT])\n",
        "EXPORT_BEST_MODEL = True\n",
        "if EXPORT_BEST_MODEL == False:\n",
        "  model_name = None\n",
        "\n",
        "USE_FINETUNED_MODEL = True\n",
        "if USE_FINETUNED_MODEL == True:\n",
        "  FINE_TUNED_MODEL_PATH = BEST_MODEL_DIR +'PATH_TO_MODEL'\n",
        "else:\n",
        "  FINE_TUNED_MODEL_PATH = None\n",
        "# Directly during training\n",
        "CRF_PREPROCESSING = False\n",
        "\n",
        "CAM_MASKS = False\n",
        "CRF_ON_CAM_MASK = False\n",
        "\n",
        "IOU_THRESHOLD = 0.3\n",
        "MASK_OCCUPANCY_THRESHOLD = 0.1\n",
        "DOUBLE_LOSS = False\n",
        "BASE_ALPHA = 0\n",
        "if DOUBLE_LOSS == False:\n",
        "  ALPHA = None\n",
        "else:\n",
        "  ALPHA = 0.05\n",
        "ENCODER = 'vgg16'\n",
        "DECODER = 'Unet'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = [\"Background\", \"Finding\"]\n",
        "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "DEVICE = 'cuda'\n",
        "OPTIMIZER = \"Adam\"\n",
        "LEARNING_RATE = 0.0001\n",
        "# Whether to use learning rate scheduling and which one\n",
        "LEARNING_RATE_SCHEDULING = True\n",
        "SCHEDULE_TYPE = \"STEP\"\n",
        "WEIGHT_DECAY = 0\n",
        "\n",
        "CAM_STD_FROM_MAX = 2.5\n",
        "\n",
        "STEP_SIZE = 5\n",
        "GAMMA = 0.5\n",
        "\n",
        "LOSS = \"CrossEntropyLoss\"\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 25\n",
        "WARMUP_EPOCHS = 0\n",
        "assert N_EPOCHS > WARMUP_EPOCHS, AssertionError(\"Warmup epochs can't be bigger than total number of epochs\")\n",
        "START_EPOCH = 0\n",
        "# Return intermediate results & Plot losses\n",
        "PER_X_BATCH = 1\n",
        "PER_X_EPOCH = 2\n",
        "PER_X_EPOCH_PLOT = 1\n",
        "\n",
        "# Mode for model name\n",
        "MODE = \"Unet_colonoscopy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6pmbKgsLcCE"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LcPdlCqLfyb",
        "outputId": "717b48bb-9f87-49e9-d484-d2bcf55de15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found data\n"
          ]
        }
      ],
      "source": [
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(\"Couldn't find data\")\n",
        "else:\n",
        "    print(\"Found data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hKksy6HOzSm"
      },
      "outputs": [],
      "source": [
        "# Transform images and masks\n",
        "# Convert to tensor before export\n",
        "img_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        #  transforms.Resize(size, interpolation=transforms.functional.InterpolationMode.NEAREST),\n",
        "         # MUST BE CORRECTED\n",
        "        transforms.Normalize((0.4040, 0.4368, 0.4569), (0.2812, 0.2678, 0.2710))\n",
        "        ])\n",
        "\n",
        "make_tensor = transforms.Compose(\n",
        "        [transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQji2mXQSt3V"
      },
      "outputs": [],
      "source": [
        "image_files = return_files_in_directory(DATA_DIR + \"/Original\", \".tif\")\n",
        "mask_files = return_files_in_directory(DATA_DIR + \"/Ground Truth\", \".tif\")\n",
        "box_files = return_files_in_directory(DATA_DIR + \"/boxmasks\", \".png\")\n",
        "sp_crf_files = return_files_in_directory(DATA_DIR + \"SP_CRF_FILES\", \".png\")\n",
        "embedding_files = return_files_in_directory(DATA_DIR + \"ROBUST_EMBEDDING_MASKS\", \".png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysXQrmq4YIiL"
      },
      "outputs": [],
      "source": [
        "# For sorting images and masks\n",
        "# https://nedbatchelder.com/blog/200712/human_sorting.html\n",
        "import re\n",
        "\n",
        "def tryint(s):\n",
        "    \"\"\"\n",
        "    Return an int if possible, or `s` unchanged.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "\n",
        "def alphanum_key(s):\n",
        "    \"\"\"\n",
        "    Turn a string into a list of string and number chunks.\n",
        "\n",
        "    >>> alphanum_key(\"z23a\")\n",
        "    [\"z\", 23, \"a\"]\n",
        "\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def human_sort(l):\n",
        "    \"\"\"\n",
        "    Sort a list in the way that humans expect.\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXiuC08UakQf"
      },
      "outputs": [],
      "source": [
        "human_sort(image_files)\n",
        "human_sort(mask_files)\n",
        "human_sort(box_files)\n",
        "human_sort(crf_files)\n",
        "human_sort(sp_crf_files)\n",
        "human_sort(embedding_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MV1mvZEbA5m"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# TODO: Check if the random state is persistant across restarts\n",
        "if TRAINING_INPUT == \"Boxes\":\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, box_files, test_size=0.1, random_state=1)\n",
        "elif TRAINING_INPUT == \"sp_crf_masks\":\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, sp_crf_files, test_size=0.1, random_state=1)\n",
        "elif TRAINING_INPUT == \"embedding_masks\":\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, embedding_files, test_size=0.1, random_state=1)\n",
        "else:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, mask_files, test_size=0.1, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11111, random_state=1) # 0.1111 x 0.9 = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vyLMnOx7UF0",
        "outputId": "db46cb3f-aadf-4351-8f38-eca3b9264002"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'embedding_masks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "TRAINING_INPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-9ToGwITu01"
      },
      "outputs": [],
      "source": [
        "# Eval on ground truth masks\n",
        "if EVAL_ON_MASKS == True:\n",
        "  if TRAINING_INPUT == \"Boxes\":\n",
        "      y_val = [i.replace(\"boxmasks\", 'Ground Truth').replace('png', 'tif') for i in y_val]\n",
        "      y_test = [i.replace(\"boxmasks\", 'Ground Truth').replace('png', 'tif') for i in y_test]\n",
        "  elif TRAINING_INPUT == \"sp_crf_masks\":\n",
        "      y_val = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_val]\n",
        "      y_test = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_test]\n",
        "  elif TRAINING_INPUT == \"embedding_masks\":\n",
        "      y_val = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_val]\n",
        "      y_test = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHRUb0R68Bb-",
        "outputId": "1bc08d99-3828-47ed-aacd-a9a92089e0d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'colonoscopy_crf_masks_adjusted_embeddings_350s_0t_it_03_mot_005_iter_1_the_005_thsp_0_foreground_embeddings_ns250_th01_sp_crf_masks_pbsxy2525_pbsrb10_pgsxy5_519.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "VALIDATE_TRAINING_TARGETS = '_'.join(y_train[0].split('/')[-4:])\n",
        "VALIDATE_TRAINING_TARGETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaBoX4Wp7CRH",
        "outputId": "6008cd9b-6f73-4979-e478-7dd094e81b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datasets_colonoscopy_Ground Truth_529.tif'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "VALIDATE_VAL_TARGETS = '_'.join(y_val[0].split('/')[-4:])\n",
        "VALIDATE_VAL_TARGETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs6npmDHx2Rc",
        "outputId": "2319b809-d8b8-4402-c7fc-236e43ca0399"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datasets_colonoscopy_Ground Truth_494.tif'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "VALIDATE_TEST_TARGETS = '_'.join(y_test[0].split('/')[-4:])\n",
        "VALIDATE_TEST_TARGETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fpxv9odPdnc0"
      },
      "outputs": [],
      "source": [
        "class Colonoscopy_Dataset(Dataset):\n",
        "    def __init__(self, X, Y, img_transform=img_transform, limit_dataset_size=None):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.img_transform = img_transform\n",
        "        self.limit_dataset_size = limit_dataset_size\n",
        "    def __len__(self):\n",
        "      if self.limit_dataset_size is not None: \n",
        "        return self.limit_dataset_size\n",
        "      else:\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # load image\n",
        "        img = imread(self.X[index])\n",
        "        # load mask\n",
        "        if \".tif\" in self.Y[index]:\n",
        "          mask = torch.tensor(imread(self.Y[index])).long()\n",
        "        elif \".png\" in self.Y[index]:\n",
        "          mask = torch.Tensor(np.array(Image.open(self.Y[index]))).long()\n",
        "        mask[mask>0] = 1\n",
        "        img_org = img\n",
        "        img_transformed = self.img_transform(img)\n",
        "        return img_transformed, mask, img_org"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0MhCW9Efx5g"
      },
      "outputs": [],
      "source": [
        "train_dataset = Colonoscopy_Dataset(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    limit_dataset_size=256\n",
        ")\n",
        "\n",
        "test_dataset = Colonoscopy_Dataset(\n",
        "    X_test, \n",
        "    y_test,\n",
        "    # limit_dataset_size=64\n",
        ")\n",
        "\n",
        "val_dataset = Colonoscopy_Dataset(\n",
        "    X_val, \n",
        "    y_val,\n",
        "    # limit_dataset_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVG5UTDDguSN"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IL1yN9WkxdP"
      },
      "source": [
        "## Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-t7QoYLiHQk",
        "outputId": "99bb0a09-2efe-4ec9-83e9-0a86651bc6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "if DECODER == 'Unet':\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER, \n",
        "        encoder_weights=ENCODER_WEIGHTS, \n",
        "        classes=len(CLASSES), \n",
        "        activation=ACTIVATION,\n",
        "        )\n",
        "    \n",
        "elif DECODER == 'DeepLabV3+':\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER, \n",
        "        encoder_weights=ENCODER_WEIGHTS, \n",
        "        classes=len(CLASSES), \n",
        "        activation=ACTIVATION,\n",
        "        )\n",
        "\n",
        "\n",
        "if OPTIMIZER == \"SGD\":\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "if OPTIMIZER == \"Adam\":\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "  # Learning rate scheduling\n",
        "if LEARNING_RATE_SCHEDULING == True and SCHEDULE_TYPE == \"STEP\":\n",
        "  scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA, verbose=True)\n",
        "elif LEARNING_RATE_SCHEDULING == True and SCHEDULE_TYPE == \"EXPONENTIAL\":\n",
        "  STEP_SIZE = 'Not needed'\n",
        "  scheduler = ExponentialLR(optimizer, gamma=GAMMA, verbose=True)\n",
        "elif LEARNING_RATE_SCHEDULING == False:\n",
        "  STEP_SIZE = 'No scheduling'\n",
        "  GAMMA = 'No scheduling'\n",
        "  SCHEDULE_TYPE = 'No scheduling'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-DJySqr_KNT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from torchmetrics import JaccardIndex\n",
        "import time\n",
        "\n",
        "label_colors = np.array(\n",
        "   [\n",
        "    (0,0,0),\n",
        "    (128,128,128)\n",
        "    ]\n",
        ")\n",
        "\n",
        "jaccard = JaccardIndex(num_classes=len(CLASSES), reduction='elementwise_mean').to(device)\n",
        "\n",
        "def return_batch_information(org_img, argmax_prediction, label, index, class_list=CLASSES, label_colors=label_colors):\n",
        "    nc = len(class_list)\n",
        "    if org_img.shape[0] > 1:\n",
        "        rgb_pred = Image.fromarray(decode_segmap(\n",
        "            argmax_prediction[index, :, :].squeeze(), label_colors, nc\n",
        "        ))\n",
        "        label_image = Image.fromarray(decode_segmap(\n",
        "            label[index, :, :].detach().cpu().squeeze().numpy(), label_colors, nc\n",
        "            ))\n",
        "        show_image = ToPILImage()(org_img[index,:,:,:].permute(2,0,1).cpu().detach().squeeze())\n",
        "        # Ensure same encoding\n",
        "        background = show_image.convert(\"RGBA\")\n",
        "        overlaylabel = label_image.convert(\"RGBA\")\n",
        "        overlaypred = rgb_pred.convert(\"RGBA\")\n",
        "        label_with_image = Image.blend(background, overlaylabel, 0.5)\n",
        "        # Create new image from overlap and make overlay 50 % transparent\n",
        "        prediction_with_image = Image.blend(background, overlaypred, 0.5)\n",
        "        visualize(org_img=show_image, prediction_with_image=prediction_with_image, label_with_image=label_with_image)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "# # Setup date for model name\n",
        "from datetime import date\n",
        "today = date.today()\n",
        "datestring = today.strftime(\"%Y-%m-%d\")\n",
        "# Instantiate accuracy and loss tracker\n",
        "best_train_loss = float('inf')\n",
        "best_valid_loss = float('inf')\n",
        "best_valid_iou = 0\n",
        "best_train_iou = 0\n",
        "\n",
        "# Build dataframe to collect loss and metric data\n",
        "df_train = pd.DataFrame(columns=['epoch', 'loss', 'avg_loss', 'mean_iou'])\n",
        "df_val = pd.DataFrame(columns=['epoch', 'loss', 'avg_loss', 'mean_iou'])\n",
        "# # Determine column types train\n",
        "# Dummy entry to prevent visualization bug that large values are plotted as zero\n",
        "if LOSS == \"CrossEntropyLoss\":\n",
        "    criterion = CrossEntropyLoss()\n",
        "    criterion_double = CrossEntropyLoss()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxUmN4FBDM2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11c879c-ebc0-419f-d4d8-3b6c6598fda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model sucessfully loaded\n"
          ]
        }
      ],
      "source": [
        "if USE_FINETUNED_MODEL == True:\n",
        "  previous_model = FINE_TUNED_MODEL_PATH\n",
        "  model = smp.Unet(\n",
        "      encoder_name=ENCODER, \n",
        "      classes=len(CLASSES), \n",
        "      activation=ACTIVATION,\n",
        "  )\n",
        "  model.load_state_dict(torch.load(previous_model))\n",
        "  model.eval()\n",
        "  model.cuda()\n",
        "  print('model sucessfully loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__uaCsdJ95b1"
      },
      "source": [
        "## CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL6Yi2i894U_"
      },
      "outputs": [],
      "source": [
        "jaccard_crf = JaccardIndex(num_classes=len(CLASSES), reduction='None', ignore_index=0).to(device)\n",
        "\n",
        "def crf(img_org, mask, pb_sxy=(25,25), pb_srgb=(10,10,10), pg_sxy=(5,5)):\n",
        "    img_np = img_org.cpu().detach().numpy()\n",
        "    # Skip background class\n",
        "    mask_np = mask.cpu().detach().numpy() * 255\n",
        "    mask_np = mask_np.astype(np.uint8)\n",
        "    mask_np[mask_np > 0] = 145\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "\n",
        "    not_mask = np.invert(mask_np)\n",
        "    not_mask = np.expand_dims(not_mask, axis=2)\n",
        "    mask_np_processed = np.expand_dims(mask_np, axis=2)\n",
        "\n",
        "    im_softmax = np.concatenate([not_mask, mask_np_processed], axis=2)\n",
        "    im_softmax = im_softmax / 255.0\n",
        "\n",
        "    gauss_img = cv2.GaussianBlur(img_np, (31, 31), 0)\n",
        "\n",
        "    bilat_img = cv2.bilateralFilter(img_np, d=10, sigmaColor=80, sigmaSpace=80)\n",
        "\n",
        "    n_classes = 2\n",
        "    feat_first = im_softmax.transpose((2, 0, 1)).reshape((n_classes,-1))\n",
        "    unary = unary_from_softmax(feat_first)\n",
        "    unary = np.ascontiguousarray(unary)\n",
        "    img_np = img_np.copy(order='C')\n",
        "\n",
        "    d = dcrf.DenseCRF2D(img_np.shape[1], img_np.shape[0], n_classes)\n",
        "\n",
        "    d.setUnaryEnergy(unary)\n",
        "    d.addPairwiseGaussian(sxy=pg_sxy, compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "\n",
        "    d.addPairwiseBilateral(sxy=pb_sxy, srgb=pb_srgb, rgbim=img_np, compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "    Q = d.inference(10)\n",
        "    res = np.argmax(Q, axis=0).reshape((img_np.shape[0], img_np.shape[1]))\n",
        "    # mutliply by one because this is the class index\n",
        "    res *= 1\n",
        "    return torch.from_numpy(res).type(torch.IntTensor)\n",
        "\n",
        "def process_batch_crf(img_org, mask):\n",
        "    if img_org.dim() > 3:\n",
        "        batch = torch.zeros(mask.shape, dtype=torch.int64).to(device)\n",
        "        for i in range(len(img_org)):\n",
        "            pseudomask = crf(img_org[i], mask[i]).to(device)\n",
        "            batch[i,:,:] = pseudomask\n",
        "        return batch\n",
        "    else:\n",
        "        return crf(img_org, mask).to(device)\n",
        "    \n",
        "\n",
        "def pass_pseudomask_or_ground_truth(masks, pseudomasks, iou_threshold=IOU_THRESHOLD, mask_occupancy_threshold=MASK_OCCUPANCY_THRESHOLD, device=device, IoU=jaccard_crf):\n",
        "    if masks.dim() > 2:\n",
        "        batch = torch.zeros(masks.shape, dtype=torch.float32).to(device)\n",
        "        pseudomasks_count = 0\n",
        "        for i in range(masks.shape[0]):\n",
        "            total_mask_occupancy = torch.count_nonzero(masks[i]) / (masks[i].shape[0] * masks[i].shape[1])\n",
        "            if IoU(pseudomasks[i].unsqueeze(0), masks[i].unsqueeze(0)) < iou_threshold or total_mask_occupancy < mask_occupancy_threshold:\n",
        "                batch[i] = masks[i]\n",
        "            else: \n",
        "                batch[i] += pseudomasks[i]\n",
        "                pseudomasks_count += 1\n",
        "        print(f\"Of {masks.shape[0]} masks {pseudomasks_count} were used\")\n",
        "        return batch\n",
        "    else:\n",
        "        total_mask_occupancy = torch.count_nonzero(masks) / (masks.shape[0] * masks.shape[1])\n",
        "        if IoU(pseudomasks.unsqueeze(0), masks.unsqueeze(0)) < iou_threshold or total_mask_occupancy < mask_occupancy_threshold:\n",
        "            return masks\n",
        "        else:\n",
        "            return pseudomasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyH0II34dNVc"
      },
      "outputs": [],
      "source": [
        "# NEEDS FIXING BECAUSE OF NEW BATCH PROCESSING FUNCTION ABOVE\n",
        "\n",
        "def export_crf_masks_for_train_data(dataset, export_path=\"/content/gdrive/MyDrive/University/FEA_Internship/data/VOCdevkit/VOC2012/datasets/colonoscopy/crf_masks/adjusted_it_03_mot_005_pbsxy2525_pbsrb10_pgsxy5\"):\n",
        "        images = dataset.X\n",
        "        masks = dataset.Y\n",
        "        for i, _ in tqdm(enumerate(images)):\n",
        "            # load image\n",
        "            img = torch.tensor(imread(_))\n",
        "            # load mask\n",
        "            if \".tif\" in masks[i]:\n",
        "              mask = torch.tensor(imread(masks[i])).long()\n",
        "            elif \".png\" in masks[i]:\n",
        "              mask = torch.Tensor(np.array(Image.open(masks[i]))).long()\n",
        "            mask[mask>0] = 1    \n",
        "            img, mask = img.to(device), mask.to(device)\n",
        "            pseudomask = process_batch_crf(img, mask)\n",
        "            pseudomask = pass_pseudomask_or_ground_truth(mask, pseudomask)\n",
        "            pseudomask = Image.fromarray(np.uint8(pseudomask.cpu().detach() * 255) , 'L')\n",
        "            output_path_mask = (\n",
        "            export_path + \"/\" + _.split('/')[-1]\n",
        "            ).replace(\"tif\", \"png\")\n",
        "            pseudomask.save(output_path_mask, quality=100, subsampling=0)\n",
        "\n",
        "# export_crf_masks_for_train_data(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E66GTjCPy-aO"
      },
      "source": [
        "## Superpixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bozYMgpHzAMZ"
      },
      "outputs": [],
      "source": [
        "from numpy.core.numeric import count_nonzero\n",
        "from skimage.segmentation import slic\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "def create_superpixel_mask(argmax_prediction_per_class, image, threshold = 0.50, class_indx=1,  N_SEGMENTS=200, compactness=10, sigma=1, start_label=1, device=device):\n",
        "    # get superpixels\n",
        "    image = image.cpu().detach().numpy()\n",
        "    all_superpixels_mask = torch.from_numpy(slic(image, n_segments=N_SEGMENTS, compactness=compactness, sigma=sigma, start_label=start_label))\n",
        "    hadamard = all_superpixels_mask.to(device) * argmax_prediction_per_class.to(device)\n",
        "    overlap = (hadamard / class_indx).type(torch.IntTensor)\n",
        "    # Instantiate base mask\n",
        "    base_mask = torch.zeros(overlap.shape)\n",
        "    # Get numbers to list, start from second element because first is 0 \n",
        "    relevant_superpixels = torch.unique(overlap).int().tolist()[1:]\n",
        "    for superpixel in relevant_superpixels:\n",
        "      temp = overlap.clone()\n",
        "      org = all_superpixels_mask.clone()\n",
        "    #   # Check how many are non-zero in superpixel mask\n",
        "      temp[temp != superpixel] = 0\n",
        "      org[org != superpixel] = 0\n",
        "      # Check how many are non-zero in overlap\n",
        "      # Determine share of pixels\n",
        "      share = torch.count_nonzero(temp).item() / torch.count_nonzero(org).item()\n",
        "      # Add superpixel as ones to base mask if share is over threshold\n",
        "      if share > threshold:\n",
        "        # bring org values to one\n",
        "        org = org / torch.unique(org)[1].item()\n",
        "        base_mask += org\n",
        "    # make values in base_mask equal the class value\n",
        "    base_mask = base_mask * class_indx\n",
        "    return base_mask.type(torch.IntTensor)\n",
        "\n",
        "def precomputed_create_superpixel_mask(argmax_prediction_per_class, boundaries, threshold = 0.50, class_indx=4):\n",
        "    # get superpixels\n",
        "    with torch.no_grad():\n",
        "        pred_cleaned = argmax_prediction_per_class.squeeze(0).clone()\n",
        "        pred_cleaned[pred_cleaned != class_indx] = 0\n",
        "        hadamard = boundaries * pred_cleaned\n",
        "        overlap = (hadamard / class_indx).type(torch.IntTensor)\n",
        "        # Instantiate base mask\n",
        "        base_mask = torch.zeros(overlap.shape).to(DEVICE)\n",
        "        # Get numbers to list, start from second element because first is 0 \n",
        "        relevant_superpixels = torch.unique(overlap).int().tolist()[1:]\n",
        "        for superpixel in relevant_superpixels:\n",
        "          temp = overlap.clone()\n",
        "          org = boundaries.clone()\n",
        "          # Check how many are non-zero in superpixel mask\n",
        "          temp[temp != superpixel] = 0\n",
        "          org[org != superpixel] = 0\n",
        "          # Check how many are non-zero in overlap\n",
        "          # Determine share of pixels\n",
        "          share = torch.count_nonzero(temp).item() / torch.count_nonzero(org).item()\n",
        "          # Add superpixel as ones to base mask if share is over threshold\n",
        "          if share > threshold:\n",
        "            # bring org values to one\n",
        "            org = org / torch.unique(org)[1].item()\n",
        "            base_mask += org\n",
        "        # make values in base_mask equal the class value\n",
        "        base_mask = base_mask * class_indx\n",
        "        base_mask[(base_mask != 0) & (base_mask != class_indx)] = class_indx\n",
        "        return base_mask.type(torch.IntTensor)\n",
        "\n",
        "def visualize_superpixels(boundaries, **images):\n",
        "    \"\"\"Plot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(25, 10))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(\" \".join(name.split(\"_\")).title())\n",
        "        plt.imshow(mark_boundaries(image, boundaries))\n",
        "    plt.show()\n",
        "\n",
        "def export_superpixel_crf_masks_for_train_data(dataset, export_path=\"path\"):\n",
        "      images = dataset.X\n",
        "      masks = dataset.Y\n",
        "      for i, _ in tqdm(enumerate(images)):\n",
        "          # load image\n",
        "          img = torch.tensor(imread(_))\n",
        "          # load mask\n",
        "          if \".tif\" in masks[i]:\n",
        "            mask = torch.tensor(imread(masks[i])).long()\n",
        "          elif \".png\" in masks[i]:\n",
        "            mask = torch.Tensor(np.array(Image.open(masks[i]))).long()\n",
        "          mask[mask>0] = 1    \n",
        "          sp_mask = create_superpixel_mask(mask, img, N_SEGMENTS=200, threshold=0.60)\n",
        "          img, sp_mask = img.to(device), sp_mask.to(device)\n",
        "          pseudomask = process_batch_crf(img, sp_mask)\n",
        "          pseudomask = pass_pseudomask_or_ground_truth(mask.to(device), pseudomask)\n",
        "          pseudomask = Image.fromarray(np.uint8(pseudomask.cpu().detach() * 255) , 'L')\n",
        "          output_path_mask = (\n",
        "          export_path + \"/\" + _.split('/')[-1]\n",
        "          ).replace(\"tif\", \"png\")\n",
        "          pseudomask.save(output_path_mask, quality=100, subsampling=0)\n",
        "# export_superpixel_crf_masks_for_train_data(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC1r4UwOHV1k"
      },
      "source": [
        "## Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M8byXbeRl2C"
      },
      "outputs": [],
      "source": [
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# useful function from prior experiments\n",
        "# used to show image alongside label or prediction mask\n",
        "def show_grad_cam_on_img(org_image, cam):\n",
        "    rgb_img = np.float32(org_image) / 255\n",
        "    cam_rgb = show_cam_on_image(rgb_img, cam, use_rgb=True)\n",
        "    return cam_rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qmak0Op98Cb"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uueHQmbClIHb"
      },
      "outputs": [],
      "source": [
        "# Write params to ML Flow\n",
        "# Start measuring time to train\n",
        "# End MLflow run if there is one\n",
        "mlflow.end_run()\n",
        "\n",
        "# Log params for ML flow\n",
        "params = {'state': STATE,\n",
        "          'encoder': ENCODER,\n",
        "          'decoder': DECODER,\n",
        "          'activation': ACTIVATION,\n",
        "          'weights': ENCODER_WEIGHTS,\n",
        "          'batch_size': train_loader.batch_size, \n",
        "          'optimizer_name':OPTIMIZER, \n",
        "          'learning_rate': optimizer.defaults['lr'],\n",
        "          'learning_rate_scheduler': SCHEDULE_TYPE,\n",
        "          'step_size': STEP_SIZE,\n",
        "          'gamma': GAMMA,\n",
        "          'weight_decay': optimizer.defaults['weight_decay'], \n",
        "          'train_dataset_size': len(train_loader.dataset), \n",
        "          'valid_dataset_size': len(val_loader.dataset),\n",
        "          'eval_on_test_set': EVAL_ON_MASKS,\n",
        "          'training_input': TRAINING_INPUT, \n",
        "          'crf_preprocessing_on_the_fly': CRF_PREPROCESSING,\n",
        "          'cam_masks': CAM_MASKS,\n",
        "          'warmup_epochs': WARMUP_EPOCHS,\n",
        "          'mask_occupancy_threshold': MASK_OCCUPANCY_THRESHOLD,\n",
        "          'iou_threshold': IOU_THRESHOLD,\n",
        "          'cam_std_from_max': CAM_STD_FROM_MAX,\n",
        "          'crf_on_cam_mask': CRF_ON_CAM_MASK,\n",
        "          'raise_incrementally': DOUBLE_LOSS, \n",
        "          'alpha': ALPHA,\n",
        "          'use_fine_tuned_model': USE_FINETUNED_MODEL,\n",
        "          'fine_tuned_model_path': FINE_TUNED_MODEL_PATH,\n",
        "          'export_best_model': EXPORT_BEST_MODEL,\n",
        "          'epochs': N_EPOCHS,\n",
        "          'y_train': VALIDATE_TRAINING_TARGETS,\n",
        "          'y_test': VALIDATE_TEST_TARGETS,\n",
        "          'y_val': VALIDATE_VAL_TARGETS,\n",
        "          }\n",
        "\n",
        "# Build the run\n",
        "number_of_runs_in_experiment = len(mlflow.search_runs())\n",
        "run_name = \"_\".join([STATE, DECODER, ENCODER, str(number_of_runs_in_experiment+1)])\n",
        "\n",
        "# with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name):\n",
        "with mlflow.start_run(run_name=run_name):\n",
        "    # Write params to ML Flow\n",
        "    mlflow.log_params(params)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    train_start_time = time.time()\n",
        "    train_iou_score = torch.tensor([0])\n",
        "    early_stopped = 0 \n",
        "    for epoch in range(START_EPOCH, N_EPOCHS):\n",
        "        model.train()\n",
        "        if DOUBLE_LOSS == True and epoch > WARMUP_EPOCHS:\n",
        "          BASE_ALPHA = BASE_ALPHA + ALPHA\n",
        "          if BASE_ALPHA > 1:\n",
        "            BASE_ALPHA = 1\n",
        "          print(f\"Base alpha increase to {BASE_ALPHA}\")\n",
        "        batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "            for train_inputs, train_labels, train_org_images in tepoch:\n",
        "                batch += 1\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                train_inputs, train_labels, train_org_images = train_inputs.to(device), train_labels.to(device), train_org_images.to(device)\n",
        "                # forward\n",
        "                train_outputs = model(train_inputs).to(device)\n",
        "                out_max = torch.argmax(train_outputs, dim=1, keepdim=True)[: , -1, :, :].cpu().detach().numpy()\n",
        "                if CRF_PREPROCESSING == True:\n",
        "                  train_labels = pass_pseudomask_or_ground_truth(process_batch_crf(train_org_images, train_labels))\n",
        "                elif CAM_MASKS == True and epoch >= WARMUP_EPOCHS and DOUBLE_LOSS == False:\n",
        "                  train_labels_cam = process_batch_cam(inputs=train_inputs, org_images=train_org_images, ground_truth_masks=train_labels, output_argmax=out_max, filter_activations=True, std_from_max=CAM_STD_FROM_MAX, crf_on_cam=CRF_ON_CAM_MASK)\n",
        "                  train_loss = criterion(train_outputs, train_labels_cam.to(device))\n",
        "                  train_loss.backward()\n",
        "                elif CAM_MASKS == True and epoch >= WARMUP_EPOCHS and DOUBLE_LOSS == True:\n",
        "                  train_loss = criterion(train_outputs, train_labels)\n",
        "                  train_labels_cam = process_batch_cam(inputs=train_inputs, org_images=train_org_images, ground_truth_masks=train_labels, output_argmax=out_max, filter_activations=True, std_from_max=CAM_STD_FROM_MAX, crf_on_cam=CRF_ON_CAM_MASK)\n",
        "                  total_loss = (1-BASE_ALPHA)*train_loss + BASE_ALPHA*criterion_double(train_outputs, train_labels_cam)\n",
        "                  total_loss.backward()\n",
        "                else:\n",
        "                  # Backward\n",
        "                  train_loss = criterion(train_outputs, train_labels)\n",
        "                  train_loss.backward()\n",
        "                if epoch % PER_X_EPOCH == 0 and batch % PER_X_BATCH == 0 and CAM_MASKS == False:\n",
        "                    return_batch_information(train_org_images, out_max, train_labels, 1, CLASSES, label_colors=label_colors)\n",
        "                elif epoch % PER_X_EPOCH == 0 and batch % PER_X_BATCH == 0 and CAM_MASKS == True and epoch >= WARMUP_EPOCHS:\n",
        "                    return_batch_information(train_org_images, out_max, train_labels_cam, 1, CLASSES, label_colors=label_colors)\n",
        "\n",
        "                optimizer.step()\n",
        "                model.eval()\n",
        "                train_iou_score =  jaccard(train_outputs, train_labels).to(device).item()\n",
        "                model.train()\n",
        "                running_epoch_iou += train_iou_score\n",
        "                train_loss = float(train_loss.item())\n",
        "                running_epoch_loss += train_loss\n",
        "                # print statistics\n",
        "                tepoch.set_postfix(phase=\"Training\", loss=train_loss, iou=train_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "            train_mean_epoch_iou, train_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "        if best_train_loss > train_mean_epoch_loss:\n",
        "          best_train_loss = train_mean_epoch_loss\n",
        "        if best_train_iou < train_mean_epoch_iou:\n",
        "          best_train_iou = train_mean_epoch_iou\n",
        "        # Save results to dataframe\n",
        "        if epoch == 0:\n",
        "          train_row = {'epoch': int(epoch), 'loss': float(train_mean_epoch_loss), 'avg_loss': float(train_mean_epoch_loss),'mean_iou': float(train_mean_epoch_iou)}\n",
        "        else:\n",
        "          # Get moving average\n",
        "          train_avg = df_train['loss'].ewm(com=0.99).mean()\n",
        "          train_row = {'epoch': int(epoch), 'loss': float(train_loss), 'avg_loss': train_avg[(epoch-1)],'mean_iou': train_mean_epoch_iou}\n",
        "        \n",
        "        # Send logs to ML flow\n",
        "        mlflow.log_metric(key=\"train_loss\", value=train_mean_epoch_loss, step=epoch)\n",
        "        mlflow.log_metric(key=\"train_iou\", value=train_mean_epoch_iou, step=epoch)\n",
        "        df_train = df_train.append(train_row, ignore_index=True)\n",
        "        # Decay Learning Rate at x steps\n",
        "        if LEARNING_RATE_SCHEDULING == True:\n",
        "            scheduler.step()\n",
        "        # Delete variables to free memory\n",
        "        del running_epoch_iou, running_epoch_loss, train_loss, train_iou_score\n",
        "\n",
        "    ### Running validation loop\n",
        "        batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            with tqdm(val_loader, unit=\"batch\") as tepoch:\n",
        "                for val_inputs, val_labels, val_org_images in tepoch:\n",
        "                    batch += 1\n",
        "                    tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                    val_inputs, val_labels, val_org_images = val_inputs.to(device), val_labels.to(device), val_org_images.to(device)\n",
        "                    # forward \n",
        "                    val_outputs = model(val_inputs)\n",
        "                    # Collect metrics\n",
        "                    val_iou_score = jaccard(val_outputs, val_labels).item()\n",
        "                    val_loss = criterion(val_outputs, val_labels).item()\n",
        "                    # Collect data for dataframe\n",
        "                    running_epoch_iou += val_iou_score\n",
        "                    running_epoch_loss += val_loss\n",
        "                    # print statistics\n",
        "                    tepoch.set_postfix(phase=\"Validation\", loss=val_loss, iou=val_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "            val_mean_epoch_iou, val_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "            # Save results to dataframe\n",
        "            if epoch == 0:\n",
        "              val_row = {'epoch': int(epoch), 'loss': float(val_mean_epoch_loss), 'avg_loss': float(val_mean_epoch_loss),'mean_iou': val_mean_epoch_iou}\n",
        "            else:\n",
        "              val_avg = df_val['loss'].ewm(com=0.99).mean()\n",
        "              val_row = {'epoch': int(epoch), 'loss': float(val_loss), 'avg_loss': val_avg[(epoch-1)],'mean_iou': val_mean_epoch_iou}\n",
        "            df_val = df_val.append(val_row, ignore_index=True)\n",
        "            if best_valid_loss > val_mean_epoch_loss:\n",
        "              # Update best metrics\n",
        "              best_valid_loss = val_mean_epoch_loss\n",
        "            if best_valid_iou < val_mean_epoch_iou: \n",
        "              best_valid_iou = val_mean_epoch_iou\n",
        "              best_model = copy.deepcopy(model)\n",
        "              if epoch > 2 and EXPORT_BEST_MODEL == True:\n",
        "                model_name = \"_\".join([datestring, STATE, MODE, DECODER, OPTIMIZER, LOSS, ENCODER, str(len(train_dataset)), \"images\", LOSS, \"loss\", str(best_valid_loss).replace(\".\",\"_\"), \"iou\", str(val_mean_epoch_iou), \"epoch\", str(epoch), \".pth\"])\n",
        "                # save model\n",
        "                path = os.path.join(BEST_MODEL_DIR, model_name)\n",
        "                torch.save(best_model.state_dict(), path)\n",
        "                print(f'Model saved! Name is {model_name}')          \n",
        "            if epoch % PER_X_EPOCH_PLOT == 0:\n",
        "              plt.plot(df_train['epoch'], df_train['avg_loss'], label = \"Train Loss\")\n",
        "              plt.plot(df_val['epoch'], df_val['avg_loss'], label = \"Valid Loss\")\n",
        "              plt.plot(df_val['epoch'], df_val['mean_iou'], label = \"Mean IoU\")\n",
        "              plt.legend()\n",
        "              plt.title('Performance')\n",
        "              plot = plt.gcf()\n",
        "              plt.show()\n",
        "            train_df_name = \"_\".join([datestring, \"train\", MODE, DECODER, OPTIMIZER, LOSS, ENCODER, str(len(train_dataset)), \"images\", \".csv\"])\n",
        "            valid_df_name = \"_\".join([datestring, \"valid\", MODE, DECODER, OPTIMIZER, LOSS, ENCODER, str(len(train_dataset)), \"images\", \".csv\"])\n",
        "            df_train.to_csv(os.path.join(EXPORT_CSV_DIR, train_df_name))\n",
        "            df_val.to_csv(os.path.join(EXPORT_CSV_DIR, valid_df_name))\n",
        "            mlflow.log_metric(key=\"valid_loss\", value=val_mean_epoch_loss, step=epoch)\n",
        "            mlflow.log_metric(key=\"valid_iou\", value=val_mean_epoch_iou, step=epoch)\n",
        "            if epoch > 5:\n",
        "                last_runs = df_train['loss'][-5:]\n",
        "                # Get min and max of that window\n",
        "                min_loss_last_runs = last_runs.min()\n",
        "                max_loss_last_runs = last_runs.max()\n",
        "                difference = max_loss_last_runs - min_loss_last_runs\n",
        "                if difference < 0.001:\n",
        "                  print(\"Stopped Training because it doesn't improve anymore.\")\n",
        "                  train_end_time = time.time()\n",
        "                  # Get minutes and seconds to write to ML flow\n",
        "                  train_mins, train_secs = epoch_time(train_start_time, train_end_time)\n",
        "                  mlflow.log_param(\"train_time\", f\"{train_mins} min, {train_secs} sec\")\n",
        "                  # Get run information and return to window\n",
        "                  run = mlflow.active_run()\n",
        "                  print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "                  # End run and get status\n",
        "                  mlflow.log_metric(key=\"best_valid_loss\", value=best_valid_loss)\n",
        "                  mlflow.log_metric(key=\"best_train_loss\", value=best_train_loss)\n",
        "                  mlflow.log_metric(key=\"best_valid_iou\", value=best_valid_iou)\n",
        "                  mlflow.log_metric(key=\"best_train_iou\", value=best_train_iou)\n",
        "                  batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "                  best_model.eval()\n",
        "                  with torch.no_grad():\n",
        "                      with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
        "                          for test_inputs, test_labels, test_org_images in tepoch:\n",
        "                              batch += 1\n",
        "                              tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                              test_inputs, test_labels, test_org_images = test_inputs.to(device), test_labels.to(device), test_org_images.to(device)\n",
        "                              # forward \n",
        "                              test_outputs = best_model(test_inputs)\n",
        "                              # Collect metrics\n",
        "                              test_iou_score = jaccard(test_outputs, test_labels).item()\n",
        "                              test_loss = criterion(test_outputs, test_labels).item()\n",
        "                              # Collect data for dataframe\n",
        "                              running_epoch_iou += test_iou_score\n",
        "                              running_epoch_loss += test_loss\n",
        "                              # print statistics\n",
        "                              tepoch.set_postfix(phase=\"Validation\", loss=test_loss, iou=test_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "                      test_mean_epoch_iou, test_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "                      mlflow.log_metric(key=\"test_iou\", value=test_mean_epoch_iou)\n",
        "                      mlflow.log_metric(key=\"test_loss\", value=test_mean_epoch_loss)\n",
        "                  print(\"\")\n",
        "                  print(f\"Performance on test set: {val_mean_epoch_iou} IoU and {val_mean_epoch_loss} Loss\")\n",
        "                  mlflow.log_params({'best_model': model_name})\n",
        "                  mlflow.end_run()\n",
        "                  run = mlflow.get_run(run.info.run_id)\n",
        "                  print(f\"Training time was {train_mins, train_secs}\")\n",
        "                  print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "                  print(\"--\")\n",
        "\n",
        "                  # Check for any active runs\n",
        "                  print(\"Active run: {}\".format(mlflow.active_run()))\n",
        "                  early_stopped += 1\n",
        "                  break\n",
        "    if early_stopped == 0:\n",
        "        train_end_time = time.time()\n",
        "        # Get minutes and seconds to write to ML flow\n",
        "        train_mins, train_secs = epoch_time(train_start_time, train_end_time)\n",
        "        mlflow.log_param(\"train_time\", f\"{train_mins} min, {train_secs} sec\")\n",
        "        # Get run information and return to window\n",
        "        run = mlflow.active_run()\n",
        "        print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "        # End run and get status\n",
        "        mlflow.log_metric(key=\"best_valid_loss\", value=best_valid_loss)\n",
        "        mlflow.log_metric(key=\"best_train_loss\", value=best_train_loss)\n",
        "        mlflow.log_metric(key=\"best_valid_iou\", value=best_valid_iou)\n",
        "        mlflow.log_metric(key=\"best_train_iou\", value=best_train_iou)\n",
        "        batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "        best_model.eval()\n",
        "        with torch.no_grad():\n",
        "            with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
        "                for test_inputs, test_labels, test_org_images in tepoch:\n",
        "                    batch += 1\n",
        "                    tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                    test_inputs, test_labels, test_org_images = test_inputs.to(device), test_labels.to(device), test_org_images.to(device)\n",
        "                    # forward \n",
        "                    test_outputs = best_model(test_inputs)\n",
        "                    # Collect metrics\n",
        "                    test_iou_score = jaccard(test_outputs, test_labels).item()\n",
        "                    test_loss = criterion(test_outputs, test_labels).item()\n",
        "                    # Collect data for dataframe\n",
        "                    running_epoch_iou += test_iou_score\n",
        "                    running_epoch_loss += test_loss\n",
        "                    # print statistics\n",
        "                    tepoch.set_postfix(phase=\"Validation\", loss=test_loss, iou=test_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "            test_mean_epoch_iou, test_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "            mlflow.log_metric(key=\"test_iou\", value=test_mean_epoch_iou)\n",
        "            mlflow.log_metric(key=\"test_loss\", value=test_mean_epoch_loss)\n",
        "            print(\"\")\n",
        "            print(f\"Performance on test set: {test_mean_epoch_iou} IoU and {test_mean_epoch_loss} Loss\")\n",
        "        mlflow.log_params({'best_model': model_name})\n",
        "        mlflow.end_run()\n",
        "        run = mlflow.get_run(run.info.run_id)\n",
        "        print(f\"Training time was {train_mins, train_secs}\")\n",
        "        print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "        print(\"--\")\n",
        "    \n",
        "        # Check for any active runs\n",
        "        print(\"Active run: {}\".format(mlflow.active_run()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2gGGYf-Fcsw"
      },
      "outputs": [],
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-0P6WB6nJ4Z"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgHJEmOu1u9K"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from pytorch_grad_cam import GradCAM\n",
        "\n",
        "# A model wrapper that gets a resnet model and returns the features before the fully connected layer.\n",
        "class ResnetFeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ResnetFeatureExtractor, self).__init__()\n",
        "        self.model = model\n",
        "        self.feature_extractor = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
        "                \n",
        "    def __call__(self, x):\n",
        "        return self.feature_extractor(x)[:, :, 0, 0]\n",
        "        \n",
        "resnet = torchvision.models.resnet50(pretrained=True)\n",
        "resnet.eval()\n",
        "feature_extract_model = ResnetFeatureExtractor(resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JArsA9Tzrrtz"
      },
      "outputs": [],
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0)\n",
        "def get_cosine_sim_score(feat_1, feat_2, cosine_fct=cos):\n",
        "    return (torch.sum(cos(feat_1.squeeze(), feat_2.squeeze())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGdBrHXUsKWd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "feature_extract_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a7k6A7O9z8W"
      },
      "outputs": [],
      "source": [
        "def get_bbox_coordinates_one_box(tensor): \n",
        "    all_x, all_y = (tensor.squeeze() == 1).nonzero(as_tuple=True)\n",
        "    smallest_x, smallest_y = torch.min(all_x).item(), torch.min(all_y).item()\n",
        "    largest_x, largest_y = torch.max(all_x).item(), torch.max(all_y).item()\n",
        "    return (smallest_y, smallest_x), (largest_y, largest_x)\n",
        "\n",
        "def get_foreground_background_embeddings(argmax_prediction_per_class, org_img, train_input, threshold, N_SEGMENTS, class_indx=1,  compactness=10, sigma=1, start_label=1, device=device, model=feature_extract_model):\n",
        "    # get superpixels\n",
        "    org_img = org_img.cpu().detach().numpy()\n",
        "    all_superpixels_mask = torch.from_numpy(slic(org_img, n_segments=N_SEGMENTS, compactness=compactness, sigma=sigma, start_label=start_label))\n",
        "    visualize_superpixels(all_superpixels_mask.numpy(), img=org_img)\n",
        "    hadamard = all_superpixels_mask.to(device) * argmax_prediction_per_class.to(device)\n",
        "    overlap = (hadamard / class_indx).type(torch.IntTensor)\n",
        "    # Instantiate base mask\n",
        "    base_mask = torch.zeros(overlap.shape)\n",
        "    # Get numbers to list, start from second element because first is 0 \n",
        "    relevant_superpixels = torch.unique(overlap).int().tolist()[1:]\n",
        "    relevant_superpixels_thresholded = []\n",
        "    for superpixel in relevant_superpixels:\n",
        "          temp = overlap.clone()\n",
        "          org = all_superpixels_mask.clone()\n",
        "        #   # Check how many are non-zero in superpixel mask\n",
        "          temp[temp != superpixel] = 0\n",
        "          org[org != superpixel] = 0\n",
        "          # Check how many are non-zero in overlap\n",
        "          # Determine share of pixels\n",
        "          share = torch.count_nonzero(temp).item() / torch.count_nonzero(org).item()\n",
        "          # Add superpixel as ones to base mask if share is over threshold\n",
        "          if share > threshold:\n",
        "            # bring org values to one\n",
        "            relevant_superpixels_thresholded.append(superpixel)\n",
        "    background_superpixels = [i.item() for i in torch.unique(all_superpixels_mask) if i not in relevant_superpixels_thresholded]\n",
        "    foreground_embeddings = torch.zeros([len(relevant_superpixels_thresholded), 2048])\n",
        "    background_embeddings = torch.zeros([len(background_superpixels), 2048])\n",
        "    for i, superpixel in enumerate(relevant_superpixels_thresholded):\n",
        "      all_superpixels_mask_tmp = all_superpixels_mask.clone()\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp != superpixel] = 0\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp>0] = 1\n",
        "      s,l = get_bbox_coordinates_one_box(all_superpixels_mask_tmp)\n",
        "      print(f\"Org shape: {torch.Tensor(org_img).permute(2,0,1).shape}\")\n",
        "      print(f\"Input shape: {train_input.shape}\")\n",
        "      base = torch.Tensor(org_img).permute(2,0,1).clone().cpu() / 255#train_input.clone().cpu()\n",
        "      base_aspm = base.clone()\n",
        "\n",
        "      base_aspm[0,:,:] = base_aspm[0,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[1,:,:] = base_aspm[1,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[2,:,:] = base_aspm[2,:,:] * all_superpixels_mask_tmp\n",
        "      cut = base_aspm[:,s[1]:l[1],s[0]:l[0]].unsqueeze(0).to(device)\n",
        "      visualize(cut=cut.cpu().squeeze(0).permute(2,1,0))\n",
        "      with torch.no_grad():\n",
        "        feat_foreground_sp = feature_extract_model(cut)\n",
        "        foreground_embeddings[i,:] = feat_foreground_sp\n",
        "    for i, superpixel in enumerate(background_superpixels):\n",
        "      all_superpixels_mask_tmp = all_superpixels_mask.clone()\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp != superpixel] = 0\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp>0] = 1\n",
        "      s,l = get_bbox_coordinates_one_box(all_superpixels_mask_tmp)\n",
        "      base = train_input.clone().cpu()\n",
        "      base_aspm = base.clone()\n",
        "\n",
        "      base_aspm[0,:,:] = base_aspm[0,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[1,:,:] = base_aspm[1,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[2,:,:] = base_aspm[2,:,:] * all_superpixels_mask_tmp\n",
        "      cut = base_aspm[:,s[1]:l[1],s[0]:l[0]].unsqueeze(0).to(device)\n",
        "      with torch.no_grad():\n",
        "        feat_background_sp = model(cut)\n",
        "        background_embeddings[i,:] = feat_background_sp\n",
        "    return foreground_embeddings, background_embeddings, relevant_superpixels_thresholded, all_superpixels_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvexgqX8F7fD"
      },
      "outputs": [],
      "source": [
        "## Get mean embeddings\n",
        "\n",
        "# foreground_embeddings = torch.zeros([len(train_dataset), 2048])\n",
        "# backgound_embeddings = torch.zeros([len(train_dataset), 2048])\n",
        "# counter = 0\n",
        "# for epoch in range(0, 1):\n",
        "#   batch = 0\n",
        "#   with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "#     for train_inputs, train_labels, train_org_images in tepoch:\n",
        "#         batch += 1\n",
        "#         tepoch.set_description(f\"Epoch {epoch}\")\n",
        "#         train_inputs, train_labels, train_org_images = train_inputs.to(device), train_labels.to(device), train_org_images.to(device)\n",
        "#         for i in range(0,train_inputs.shape[0],1):\n",
        "#           i = 7\n",
        "#           N_SEGMENTS = 250\n",
        "#           threshold = 0.10\n",
        "#           vis = train_org_images[i].squeeze(0).permute(0,1,2).cpu()\n",
        "#           print(vis.shape)\n",
        "#           print(f\"train_input: {train_inputs.shape}\")\n",
        "#           print(f\"train_org_images: {train_org_images.shape}\")\n",
        "#           visualize(img=vis)\n",
        "#           embed_f, embed_b, rs, aspm = get_foreground_background_embeddings(train_labels[i], train_org_images[i], train_inputs[i], N_SEGMENTS=N_SEGMENTS, threshold=threshold)\n",
        "#           break\n",
        "#         break\n",
        "#   break\n",
        "          # mean_f = torch.mean(embed_f, dim=0)\n",
        "          # mean_b = torch.mean(embed_b, dim=0)\n",
        "\n",
        "          # foreground_embeddings[counter,:] += mean_f.cpu()\n",
        "          # backgound_embeddings[counter,:] += mean_b.cpu()\n",
        "          # counter += 1\n",
        "          # break\n",
        "          # if counter == 477:\n",
        "          #   print(\"if-break\")\n",
        "          #   # torch.save(foreground_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "          #   # torch.save(backgound_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "          #   break\n",
        "        # torch.save(foreground_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "        # torch.save(backgound_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "\n",
        "f = torch.load('PATH_TO_MEAN_EMBEDDING_VECTOR')\n",
        "b = torch.load('PATH_TO_MEAN_EMBEDDING_VECTOR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gngUDjKGTLw"
      },
      "outputs": [],
      "source": [
        "def assign_foreground_sp(cosine_fct, mean_foreground_embedding, mean_background_embedding, relevant_superpixels_thresholded, foreground_embeddings, threshold):\n",
        "    close_f_foreground_embeddings = []\n",
        "    for i in range(foreground_embeddings.shape[0]):\n",
        "        f_cos = cosine_fct(mean_foreground_embedding, foreground_embeddings[i])\n",
        "        b_cos = cosine_fct(mean_background_embedding, foreground_embeddings[i])\n",
        "        diff = abs(b_cos - f_cos)\n",
        "        if (f_cos > b_cos or f_cos == b_cos) and diff <= threshold:\n",
        "          close_f_foreground_embeddings.append(relevant_superpixels_thresholded[i])\n",
        "    return close_f_foreground_embeddings\n",
        "\n",
        "def scan_outer_boundary(superpixel_mask, flatten_list_fct=flatten):\n",
        "    # We will collect those superpixels which are on the outer boundary so they \n",
        "    # have a zero neighbour.\n",
        "    superpixel_mask_refined = superpixel_mask.clone()\n",
        "    outer_superpixel_rows, outer_superpixel_cols, outer_all = [], [], []\n",
        "    tuples = torch.nonzero(superpixel_mask_refined)\n",
        "    rows = torch.unique(tuples[:,0])\n",
        "    columns = torch.unique(tuples[:,1])\n",
        "    # scan over rows \n",
        "    for i in rows:\n",
        "      current_row = superpixel_mask_refined[i, :]\n",
        "      unique_non_zeroed_row = torch.unique(current_row[current_row.nonzero(as_tuple=True)], sorted=False)\n",
        "      first_superpixel = unique_non_zeroed_row[-1]\n",
        "      last_superpixel = unique_non_zeroed_row[0]\n",
        "      outer_superpixel_rows.append(first_superpixel.item())\n",
        "      outer_superpixel_rows.append(last_superpixel.item())\n",
        "    # scan over columns\n",
        "    for i in columns:\n",
        "      current_column = superpixel_mask_refined[:, i]\n",
        "      unique_non_zeroed_column = torch.unique(current_column[current_column.nonzero(as_tuple=True)], sorted=False)\n",
        "      first_superpixel = unique_non_zeroed_column[-1]\n",
        "      last_superpixel = unique_non_zeroed_column[0]\n",
        "      outer_superpixel_cols.append(first_superpixel.item())\n",
        "      outer_superpixel_cols.append(last_superpixel.item())\n",
        "    outer_all.append(outer_superpixel_cols)\n",
        "    outer_all. append(outer_superpixel_rows)\n",
        "    outer_all = flatten(outer_all)\n",
        "    return list(set(outer_all))\n",
        "\n",
        "def create_embedding_mask(train_label, \n",
        "                          train_org_image, \n",
        "                          train_input, \n",
        "                          N_SEGMENTS, \n",
        "                          threshold_embedding=0, \n",
        "                          threshold_closeness=0, \n",
        "                          mean_foreground_embedding=mean_f, \n",
        "                          mean_background_embedding=mean_b, \n",
        "                          cosine_function=get_cosine_sim_score, \n",
        "                          assign_label_based_on_closeness=assign_foreground_sp, \n",
        "                          get_foreground_background_embeddings_function=get_foreground_background_embeddings, \n",
        "                          scan_outer_pixels=True,\n",
        "                          scan_outer_superpixels_function=scan_outer_boundary,\n",
        "                          postprocess_crf=True,\n",
        "                          iter=1):\n",
        "    foreground_embeddings, background_embeddings, relevant_superpixels, all_superpixels_mask = get_foreground_background_embeddings_function(train_label, train_org_image, train_input, N_SEGMENTS=N_SEGMENTS, threshold=threshold_embedding)\n",
        "    for i in range(0, iter, 1):\n",
        "      not_in_relevant_superpixels = [i for i in torch.unique(all_superpixels_mask) if i not in relevant_superpixels]\n",
        "      embedding_mask_relevant_superpixels = all_superpixels_mask.clone()\n",
        "      for i in not_in_relevant_superpixels:\n",
        "          embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels == i] = 0\n",
        "      if scan_outer_pixels == True:\n",
        "          outer_superpixels = scan_outer_superpixels_function(embedding_mask_relevant_superpixels)\n",
        "          indexes_outer = [relevant_superpixels.index(i) for i in outer_superpixels]\n",
        "          outer_foreground_embedding = foreground_embeddings[indexes_outer, :]\n",
        "          close_foreground_outer_superpixels = assign_foreground_sp(cosine_function, mean_foreground_embedding, mean_background_embedding, relevant_superpixels_thresholded=outer_superpixels, foreground_embeddings=outer_foreground_embedding, threshold=threshold_closeness)\n",
        "          to_be_dropped = [i for i in outer_superpixels if i not in close_foreground_outer_superpixels]\n",
        "          relevant_superpixels = [i for i in relevant_superpixels if i not in to_be_dropped]\n",
        "          not_in_relevant_superpixels = [i for i in torch.unique(all_superpixels_mask) if i not in relevant_superpixels]\n",
        "          for i in not_in_relevant_superpixels:\n",
        "              embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels == i] = 0\n",
        "    embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels > 0] = 1\n",
        "    if postprocess_crf == True:\n",
        "      embedding_mask_relevant_superpixels = crf(train_org_image, embedding_mask_relevant_superpixels)\n",
        "    else:\n",
        "      embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels > 0] = 1\n",
        "      if postprocess_crf == True:\n",
        "          embedding_mask_relevant_superpixels = crf(train_org_image, embedding_mask_relevant_superpixels)\n",
        "\n",
        "    return embedding_mask_relevant_superpixels, outer_superpixels, embedding_mask_relevant_superpixels, all_superpixels_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM34i05halkn"
      },
      "outputs": [],
      "source": [
        "def export_superpixel_embedding_masks(dataset, export_path=\"PATH\", img_transform=img_transform, show_grad_cam_on_img=show_grad_cam_on_img):\n",
        "      images = dataset.X\n",
        "      masks = dataset.Y\n",
        "      for i, _ in tqdm(enumerate(images)):\n",
        "          # load image\n",
        "          img = torch.tensor(imread(_))\n",
        "          # load mask\n",
        "          if \".tif\" in masks[i]:\n",
        "            mask = torch.tensor(imread(masks[i])).long()\n",
        "          elif \".png\" in masks[i]:\n",
        "            mask = torch.Tensor(np.array(Image.open(masks[i]))).long()\n",
        "          mask[mask>0] = 1    \n",
        "          embedding_mask_outer_fct = create_embedding_mask(mask, \n",
        "                                                     img, \n",
        "                                                     img_transform(imread(_)), \n",
        "                                                     N_SEGMENTS=200, \n",
        "                                                     iter=1)\n",
        "          embedding_mask_outer_fct = pass_pseudomask_or_ground_truth(mask.to(device), embedding_mask_outer_fct.to(device), iou_threshold=0.5, mask_occupancy_threshold=0.05)\n",
        "          embedding_mask_outer_fct = pass_pseudomask_or_ground_truth(mask.to(device), embedding_mask_outer_fct)\n",
        "          overlay_1 = show_grad_cam_on_img(img, embedding_mask_outer_fct.cpu())\n",
        "          visualize(img=img.cpu(), gt_box=mask.cpu(), embedding_mask_outer_fct=embedding_mask_outer_fct.cpu(), overlay_1=overlay_1)  \n",
        "          embedding_mask_outer_fct = Image.fromarray(np.uint8(embedding_mask_outer_fct.cpu().detach() * 255) , 'L')\n",
        "          output_path_mask = (\n",
        "          export_path + \"/\" + _.split('/')[-1]\n",
        "          ).replace(\"tif\", \"png\")\n",
        "          embedding_mask_outer_fct.save(output_path_mask, quality=100, subsampling=0)\n",
        "# export_superpixel_embedding_masks(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8Qmak0Op98Cb",
        "g-0P6WB6nJ4Z"
      ],
      "name": "colonoscopy_paper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}