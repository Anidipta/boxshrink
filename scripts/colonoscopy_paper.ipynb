{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu6IqvrhJsOX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8YnIESTSJn7F",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mgroeger/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3A5ZzNyrJrSf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# path_uri = '/content/gdrive/MyDrive/University/FEA_Internship/boxshrink/ML_FLOW'\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = input('Enter your DAGsHub username: ')\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = input('Enter your DAGsHub project name: ')\n",
        "mlflow.set_registry_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] \n",
        "                        + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] \n",
        "                        + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DBOYa_w9OxwH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking Location: https://dagshub.com/michaelgroeger/testing.mlflow\n",
            "Registry Location: https://dagshub.com/michaelgroeger/testing.mlflow\n"
          ]
        }
      ],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "print(\"Tracking Location: {}\".format(mlflow.get_tracking_uri()))\n",
        "print(\"Registry Location: {}\".format(mlflow.get_registry_uri()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pnwa23gLNviy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Import custom function\n",
        "from tools import (\n",
        "    return_files_in_directory,\n",
        "    decode_segmap,\n",
        "    get_classes_from_mask,\n",
        "    rgb_to_mask,\n",
        "    visualize,\n",
        "    return_batch_information,\n",
        "    flatten,\n",
        "    human_sort\n",
        "    )\n",
        "\n",
        "from dataset import Colonoscopy_Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lv1mVFkTOE_V",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import compute_unary, unary_from_softmax\n",
        "\n",
        "from torchmetrics import JaccardIndex\n",
        "import time\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.filters import sobel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "import copy\n",
        "from tifffile import imread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "graDYsMzOkP8"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mvVcSSJkOhiq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6pmbKgsLcCE"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LcPdlCqLfyb",
        "outputId": "717b48bb-9f87-49e9-d484-d2bcf55de15a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found data\n"
          ]
        }
      ],
      "source": [
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(\"Couldn't find data\")\n",
        "else:\n",
        "    print(\"Found data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iQji2mXQSt3V",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "image_files = return_files_in_directory(DATA_DIR + \"/Original\", \".tif\")\n",
        "mask_files = return_files_in_directory(DATA_DIR + \"/Ground Truth\", \".tif\")\n",
        "box_files = return_files_in_directory(DATA_DIR + \"/boxmasks\", \".png\")\n",
        "sp_crf_files = return_files_in_directory(DATA_DIR + \"rapid_boxshrink\", \".png\")\n",
        "embedding_files = return_files_in_directory(DATA_DIR + \"robust_boxshrink\", \".png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pXiuC08UakQf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'crf_files' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-24de2f94fad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhuman_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhuman_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhuman_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrf_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhuman_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_crf_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhuman_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crf_files' is not defined"
          ]
        }
      ],
      "source": [
        "human_sort(image_files)\n",
        "human_sort(mask_files)\n",
        "human_sort(box_files)\n",
        "# human_sort(crf_files)\n",
        "# human_sort(sp_crf_files)\n",
        "# human_sort(embedding_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MV1mvZEbA5m",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# TODO: Check if the random state is persistant across restarts\n",
        "if TRAINING_INPUT == \"Boxes\":\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, box_files, test_size=0.1, random_state=1)\n",
        "elif TRAINING_INPUT == \"rapid_boxshrink\":\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, sp_crf_files, test_size=0.1, random_state=1)\n",
        "elif TRAINING_INPUT == \"robust_boxshrink\":\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, embedding_files, test_size=0.1, random_state=1)\n",
        "else:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(image_files, mask_files, test_size=0.1, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11111, random_state=1) # 0.1111 x 0.9 = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vyLMnOx7UF0",
        "outputId": "db46cb3f-aadf-4351-8f38-eca3b9264002",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'embedding_masks'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TRAINING_INPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-9ToGwITu01",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Eval on ground truth masks\n",
        "if TRAINING_INPUT == \"Boxes\":\n",
        "    y_val = [i.replace(\"boxmasks\", 'Ground Truth').replace('png', 'tif') for i in y_val]\n",
        "    y_test = [i.replace(\"boxmasks\", 'Ground Truth').replace('png', 'tif') for i in y_test]\n",
        "elif TRAINING_INPUT == \"rapid_boxshrink\":\n",
        "    y_val = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_val]\n",
        "    y_test = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_test]\n",
        "elif TRAINING_INPUT == \"robust_boxshrink\":\n",
        "    y_val = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_val]\n",
        "    y_test = [i.replace(\"INPUT_PATH\", 'Ground Truth').replace('png', 'tif') for i in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHRUb0R68Bb-",
        "outputId": "1bc08d99-3828-47ed-aacd-a9a92089e0d8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'colonoscopy_crf_masks_adjusted_embeddings_350s_0t_it_03_mot_005_iter_1_the_005_thsp_0_foreground_embeddings_ns250_th01_sp_crf_masks_pbsxy2525_pbsrb10_pgsxy5_519.png'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VALIDATE_TRAINING_TARGETS = '_'.join(y_train[0].split('/')[-4:])\n",
        "VALIDATE_TRAINING_TARGETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaBoX4Wp7CRH",
        "outputId": "6008cd9b-6f73-4979-e478-7dd094e81b2a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'datasets_colonoscopy_Ground Truth_529.tif'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VALIDATE_VAL_TARGETS = '_'.join(y_val[0].split('/')[-4:])\n",
        "VALIDATE_VAL_TARGETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs6npmDHx2Rc",
        "outputId": "2319b809-d8b8-4402-c7fc-236e43ca0399",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'datasets_colonoscopy_Ground Truth_494.tif'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VALIDATE_TEST_TARGETS = '_'.join(y_test[0].split('/')[-4:])\n",
        "VALIDATE_TEST_TARGETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0MhCW9Efx5g",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_dataset = Colonoscopy_Dataset(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    limit_dataset_size=256\n",
        ")\n",
        "\n",
        "test_dataset = Colonoscopy_Dataset(\n",
        "    X_test, \n",
        "    y_test,\n",
        "    # limit_dataset_size=64\n",
        ")\n",
        "\n",
        "val_dataset = Colonoscopy_Dataset(\n",
        "    X_val, \n",
        "    y_val,\n",
        "    # limit_dataset_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVG5UTDDguSN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IL1yN9WkxdP"
      },
      "source": [
        "## Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-t7QoYLiHQk",
        "outputId": "99bb0a09-2efe-4ec9-83e9-0a86651bc6b4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "if DECODER == 'Unet':\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER, \n",
        "        encoder_weights=ENCODER_WEIGHTS, \n",
        "        classes=len(CLASSES), \n",
        "        activation=ACTIVATION,\n",
        "        )\n",
        "    \n",
        "elif DECODER == 'DeepLabV3+':\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER, \n",
        "        encoder_weights=ENCODER_WEIGHTS, \n",
        "        classes=len(CLASSES), \n",
        "        activation=ACTIVATION,\n",
        "        )\n",
        "        \n",
        "if OPTIMIZER == \"SGD\":\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "if OPTIMIZER == \"Adam\":\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Learning rate scheduling\n",
        "if LEARNING_RATE_SCHEDULING == True and SCHEDULE_TYPE == \"STEP\":\n",
        "  scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA, verbose=True)\n",
        "elif LEARNING_RATE_SCHEDULING == True and SCHEDULE_TYPE == \"EXPONENTIAL\":\n",
        "  STEP_SIZE = 'Not needed'\n",
        "  scheduler = ExponentialLR(optimizer, gamma=GAMMA, verbose=True)\n",
        "elif LEARNING_RATE_SCHEDULING == False:\n",
        "  STEP_SIZE = 'No scheduling'\n",
        "  GAMMA = 'No scheduling'\n",
        "  SCHEDULE_TYPE = 'No scheduling'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-DJySqr_KNT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "\n",
        "label_colors = np.array(\n",
        "    [(0,0,0), (128,128,128)]\n",
        ")\n",
        "\n",
        "jaccard = JaccardIndex(num_classes=len(CLASSES), reduction='elementwise_mean').to(device)\n",
        "\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "# Setup date for model name\n",
        "from datetime import date\n",
        "today = date.today()\n",
        "datestring = today.strftime(\"%Y-%m-%d\")\n",
        "# Instantiate accuracy and loss tracker\n",
        "best_train_loss = float('inf')\n",
        "best_valid_loss = float('inf')\n",
        "best_valid_iou = 0\n",
        "best_train_iou = 0\n",
        "\n",
        "# Build dataframe to collect loss and metric data\n",
        "df_train = pd.DataFrame(columns=['epoch', 'loss', 'avg_loss', 'mean_iou'])\n",
        "df_val = pd.DataFrame(columns=['epoch', 'loss', 'avg_loss', 'mean_iou'])\n",
        "# # Determine column types train\n",
        "# Dummy entry to prevent visualization bug that large values are plotted as zero\n",
        "if LOSS == \"CrossEntropyLoss\":\n",
        "    criterion = CrossEntropyLoss()\n",
        "    criterion_double = CrossEntropyLoss()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC1r4UwOHV1k"
      },
      "source": [
        "## Additional Visualization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M8byXbeRl2C",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# useful function from prior experiments\n",
        "# used to show image alongside label or prediction mask\n",
        "def show_grad_cam_on_img(org_image, cam):\n",
        "    rgb_img = np.float32(org_image) / 255\n",
        "    cam_rgb = show_cam_on_image(rgb_img, cam, use_rgb=True)\n",
        "    return cam_rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qmak0Op98Cb"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uueHQmbClIHb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Write params to ML Flow\n",
        "# Start measuring time to train\n",
        "# End MLflow run if there is one\n",
        "mlflow.end_run()\n",
        "\n",
        "# Log params for ML flow\n",
        "params = {'state': STATE,\n",
        "          'encoder': ENCODER,\n",
        "          'decoder': DECODER,\n",
        "          'activation': ACTIVATION,\n",
        "          'weights': ENCODER_WEIGHTS,\n",
        "          'batch_size': train_loader.batch_size, \n",
        "          'optimizer_name':OPTIMIZER, \n",
        "          'learning_rate': optimizer.defaults['lr'],\n",
        "          'learning_rate_scheduler': SCHEDULE_TYPE,\n",
        "          'step_size': STEP_SIZE,\n",
        "          'gamma': GAMMA,\n",
        "          'weight_decay': optimizer.defaults['weight_decay'], \n",
        "          'train_dataset_size': len(train_loader.dataset), \n",
        "          'valid_dataset_size': len(val_loader.dataset),\n",
        "          'eval_on_test_set': EVAL_ON_MASKS,\n",
        "          'training_input': TRAINING_INPUT, \n",
        "          'mask_occupancy_threshold': MASK_OCCUPANCY_THRESHOLD,\n",
        "          'iou_threshold': IOU_THRESHOLD,\n",
        "          'export_best_model': EXPORT_BEST_MODEL,\n",
        "          'epochs': N_EPOCHS,\n",
        "          'y_train': VALIDATE_TRAINING_TARGETS,\n",
        "          'y_test': VALIDATE_TEST_TARGETS,\n",
        "          'y_val': VALIDATE_VAL_TARGETS,\n",
        "          }\n",
        "\n",
        "# Build the run\n",
        "number_of_runs_in_experiment = len(mlflow.search_runs())\n",
        "run_name = \"_\".join([STATE, DECODER, ENCODER, str(number_of_runs_in_experiment+1)])\n",
        "\n",
        "# with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name):\n",
        "with mlflow.start_run(run_name=run_name):\n",
        "    # Write params to ML Flow\n",
        "    mlflow.log_params(params)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    train_start_time = time.time()\n",
        "    train_iou_score = torch.tensor([0])\n",
        "    early_stopped = 0 \n",
        "    for epoch in range(START_EPOCH, N_EPOCHS):\n",
        "        model.train()\n",
        "        batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "            for train_inputs, train_labels, train_org_images in tepoch:\n",
        "                batch += 1\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                train_inputs, train_labels, train_org_images = train_inputs.to(device), train_labels.to(device), train_org_images.to(device)\n",
        "                # forward\n",
        "                train_outputs = model(train_inputs).to(device)\n",
        "                out_max = torch.argmax(train_outputs, dim=1, keepdim=True)[: , -1, :, :].cpu().detach().numpy()\n",
        "                # Backward\n",
        "                train_loss = criterion(train_outputs, train_labels)\n",
        "                train_loss.backward()\n",
        "                if epoch % PER_X_EPOCH == 0 and batch % PER_X_BATCH == 0:\n",
        "                    return_batch_information(train_org_images, out_max, train_labels, 1, CLASSES, label_colors=label_colors)\n",
        "                optimizer.step()\n",
        "                model.eval()\n",
        "                train_iou_score =  jaccard(train_outputs, train_labels).to(device).item()\n",
        "                model.train()\n",
        "                running_epoch_iou += train_iou_score\n",
        "                train_loss = float(train_loss.item())\n",
        "                running_epoch_loss += train_loss\n",
        "                # print statistics\n",
        "                tepoch.set_postfix(phase=\"Training\", loss=train_loss, iou=train_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "            train_mean_epoch_iou, train_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "        if best_train_loss > train_mean_epoch_loss:\n",
        "          best_train_loss = train_mean_epoch_loss\n",
        "        if best_train_iou < train_mean_epoch_iou:\n",
        "          best_train_iou = train_mean_epoch_iou\n",
        "        # Save results to dataframe\n",
        "        if epoch == 0:\n",
        "          train_row = {'epoch': int(epoch), 'loss': float(train_mean_epoch_loss), 'avg_loss': float(train_mean_epoch_loss),'mean_iou': float(train_mean_epoch_iou)}\n",
        "        else:\n",
        "          # Get moving average\n",
        "          train_avg = df_train['loss'].ewm(com=0.99).mean()\n",
        "          train_row = {'epoch': int(epoch), 'loss': float(train_loss), 'avg_loss': train_avg[(epoch-1)],'mean_iou': train_mean_epoch_iou}\n",
        "        \n",
        "        # Send logs to ML flow\n",
        "        mlflow.log_metric(key=\"train_loss\", value=train_mean_epoch_loss, step=epoch)\n",
        "        mlflow.log_metric(key=\"train_iou\", value=train_mean_epoch_iou, step=epoch)\n",
        "        df_train = df_train.append(train_row, ignore_index=True)\n",
        "        # Decay Learning Rate at x steps\n",
        "        if LEARNING_RATE_SCHEDULING == True:\n",
        "            scheduler.step()\n",
        "        # Delete variables to free memory\n",
        "        del running_epoch_iou, running_epoch_loss, train_loss, train_iou_score\n",
        "\n",
        "    ### Running validation loop\n",
        "        batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            with tqdm(val_loader, unit=\"batch\") as tepoch:\n",
        "                for val_inputs, val_labels, val_org_images in tepoch:\n",
        "                    batch += 1\n",
        "                    tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                    val_inputs, val_labels, val_org_images = val_inputs.to(device), val_labels.to(device), val_org_images.to(device)\n",
        "                    # forward \n",
        "                    val_outputs = model(val_inputs)\n",
        "                    # Collect metrics\n",
        "                    val_iou_score = jaccard(val_outputs, val_labels).item()\n",
        "                    val_loss = criterion(val_outputs, val_labels).item()\n",
        "                    # Collect data for dataframe\n",
        "                    running_epoch_iou += val_iou_score\n",
        "                    running_epoch_loss += val_loss\n",
        "                    # print statistics\n",
        "                    tepoch.set_postfix(phase=\"Validation\", loss=val_loss, iou=val_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "            val_mean_epoch_iou, val_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "            # Save results to dataframe\n",
        "            if epoch == 0:\n",
        "              val_row = {'epoch': int(epoch), 'loss': float(val_mean_epoch_loss), 'avg_loss': float(val_mean_epoch_loss),'mean_iou': val_mean_epoch_iou}\n",
        "            else:\n",
        "              val_avg = df_val['loss'].ewm(com=0.99).mean()\n",
        "              val_row = {'epoch': int(epoch), 'loss': float(val_loss), 'avg_loss': val_avg[(epoch-1)],'mean_iou': val_mean_epoch_iou}\n",
        "            df_val = df_val.append(val_row, ignore_index=True)\n",
        "            if best_valid_loss > val_mean_epoch_loss:\n",
        "              # Update best metrics\n",
        "              best_valid_loss = val_mean_epoch_loss\n",
        "            if best_valid_iou < val_mean_epoch_iou: \n",
        "              best_valid_iou = val_mean_epoch_iou\n",
        "              best_model = copy.deepcopy(model)\n",
        "              if epoch > 2 and EXPORT_BEST_MODEL == True:\n",
        "                model_name = \"_\".join([datestring, STATE, MODE, DECODER, OPTIMIZER, LOSS, ENCODER, str(len(train_dataset)), \"images\", LOSS, \"loss\", str(best_valid_loss).replace(\".\",\"_\"), \"iou\", str(val_mean_epoch_iou), \"epoch\", str(epoch), \".pth\"])\n",
        "                # save model\n",
        "                path = os.path.join(BEST_MODEL_DIR, model_name)\n",
        "                torch.save(best_model.state_dict(), path)\n",
        "                print(f'Model saved! Name is {model_name}')          \n",
        "            if epoch % PER_X_EPOCH_PLOT == 0:\n",
        "              plt.plot(df_train['epoch'], df_train['avg_loss'], label = \"Train Loss\")\n",
        "              plt.plot(df_val['epoch'], df_val['avg_loss'], label = \"Valid Loss\")\n",
        "              plt.plot(df_val['epoch'], df_val['mean_iou'], label = \"Mean IoU\")\n",
        "              plt.legend()\n",
        "              plt.title('Performance')\n",
        "              plot = plt.gcf()\n",
        "              plt.show()\n",
        "            train_df_name = \"_\".join([datestring, \"train\", MODE, DECODER, OPTIMIZER, LOSS, ENCODER, str(len(train_dataset)), \"images\", \".csv\"])\n",
        "            valid_df_name = \"_\".join([datestring, \"valid\", MODE, DECODER, OPTIMIZER, LOSS, ENCODER, str(len(train_dataset)), \"images\", \".csv\"])\n",
        "            df_train.to_csv(os.path.join(EXPORT_CSV_DIR, train_df_name))\n",
        "            df_val.to_csv(os.path.join(EXPORT_CSV_DIR, valid_df_name))\n",
        "            mlflow.log_metric(key=\"valid_loss\", value=val_mean_epoch_loss, step=epoch)\n",
        "            mlflow.log_metric(key=\"valid_iou\", value=val_mean_epoch_iou, step=epoch)\n",
        "            if epoch > 5:\n",
        "                last_runs = df_train['loss'][-5:]\n",
        "                # Get min and max of that window\n",
        "                min_loss_last_runs = last_runs.min()\n",
        "                max_loss_last_runs = last_runs.max()\n",
        "                difference = max_loss_last_runs - min_loss_last_runs\n",
        "                if difference < 0.001:\n",
        "                  print(\"Stopped Training because it doesn't improve anymore.\")\n",
        "                  train_end_time = time.time()\n",
        "                  # Get minutes and seconds to write to ML flow\n",
        "                  train_mins, train_secs = epoch_time(train_start_time, train_end_time)\n",
        "                  mlflow.log_param(\"train_time\", f\"{train_mins} min, {train_secs} sec\")\n",
        "                  # Get run information and return to window\n",
        "                  run = mlflow.active_run()\n",
        "                  print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "                  # End run and get status\n",
        "                  mlflow.log_metric(key=\"best_valid_loss\", value=best_valid_loss)\n",
        "                  mlflow.log_metric(key=\"best_train_loss\", value=best_train_loss)\n",
        "                  mlflow.log_metric(key=\"best_valid_iou\", value=best_valid_iou)\n",
        "                  mlflow.log_metric(key=\"best_train_iou\", value=best_train_iou)\n",
        "                  batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "                  best_model.eval()\n",
        "                  with torch.no_grad():\n",
        "                      with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
        "                          for test_inputs, test_labels, test_org_images in tepoch:\n",
        "                              batch += 1\n",
        "                              tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                              test_inputs, test_labels, test_org_images = test_inputs.to(device), test_labels.to(device), test_org_images.to(device)\n",
        "                              # forward \n",
        "                              test_outputs = best_model(test_inputs)\n",
        "                              # Collect metrics\n",
        "                              test_iou_score = jaccard(test_outputs, test_labels).item()\n",
        "                              test_loss = criterion(test_outputs, test_labels).item()\n",
        "                              # Collect data for dataframe\n",
        "                              running_epoch_iou += test_iou_score\n",
        "                              running_epoch_loss += test_loss\n",
        "                              # print statistics\n",
        "                              tepoch.set_postfix(phase=\"Validation\", loss=test_loss, iou=test_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "                      test_mean_epoch_iou, test_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "                      mlflow.log_metric(key=\"test_iou\", value=test_mean_epoch_iou)\n",
        "                      mlflow.log_metric(key=\"test_loss\", value=test_mean_epoch_loss)\n",
        "                  print(\"\")\n",
        "                  print(f\"Performance on test set: {val_mean_epoch_iou} IoU and {val_mean_epoch_loss} Loss\")\n",
        "                  mlflow.log_params({'best_model': model_name})\n",
        "                  mlflow.end_run()\n",
        "                  run = mlflow.get_run(run.info.run_id)\n",
        "                  print(f\"Training time was {train_mins, train_secs}\")\n",
        "                  print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "                  print(\"--\")\n",
        "\n",
        "                  # Check for any active runs\n",
        "                  print(\"Active run: {}\".format(mlflow.active_run()))\n",
        "                  early_stopped += 1\n",
        "                  break\n",
        "    if early_stopped == 0:\n",
        "        train_end_time = time.time()\n",
        "        # Get minutes and seconds to write to ML flow\n",
        "        train_mins, train_secs = epoch_time(train_start_time, train_end_time)\n",
        "        mlflow.log_param(\"train_time\", f\"{train_mins} min, {train_secs} sec\")\n",
        "        # Get run information and return to window\n",
        "        run = mlflow.active_run()\n",
        "        print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "        # End run and get status\n",
        "        mlflow.log_metric(key=\"best_valid_loss\", value=best_valid_loss)\n",
        "        mlflow.log_metric(key=\"best_train_loss\", value=best_train_loss)\n",
        "        mlflow.log_metric(key=\"best_valid_iou\", value=best_valid_iou)\n",
        "        mlflow.log_metric(key=\"best_train_iou\", value=best_train_iou)\n",
        "        batch, running_epoch_iou, running_epoch_loss = 0, 0.0, 0.0\n",
        "        best_model.eval()\n",
        "        with torch.no_grad():\n",
        "            with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
        "                for test_inputs, test_labels, test_org_images in tepoch:\n",
        "                    batch += 1\n",
        "                    tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                    test_inputs, test_labels, test_org_images = test_inputs.to(device), test_labels.to(device), test_org_images.to(device)\n",
        "                    # forward \n",
        "                    test_outputs = best_model(test_inputs)\n",
        "                    # Collect metrics\n",
        "                    test_iou_score = jaccard(test_outputs, test_labels).item()\n",
        "                    test_loss = criterion(test_outputs, test_labels).item()\n",
        "                    # Collect data for dataframe\n",
        "                    running_epoch_iou += test_iou_score\n",
        "                    running_epoch_loss += test_loss\n",
        "                    # print statistics\n",
        "                    tepoch.set_postfix(phase=\"Validation\", loss=test_loss, iou=test_iou_score, epoch_iou = running_epoch_iou / batch, epoch_loss = running_epoch_loss / batch)\n",
        "            test_mean_epoch_iou, test_mean_epoch_loss = running_epoch_iou / batch, running_epoch_loss / batch\n",
        "            mlflow.log_metric(key=\"test_iou\", value=test_mean_epoch_iou)\n",
        "            mlflow.log_metric(key=\"test_loss\", value=test_mean_epoch_loss)\n",
        "            print(\"\")\n",
        "            print(f\"Performance on test set: {test_mean_epoch_iou} IoU and {test_mean_epoch_loss} Loss\")\n",
        "        mlflow.log_params({'best_model': model_name})\n",
        "        mlflow.end_run()\n",
        "        run = mlflow.get_run(run.info.run_id)\n",
        "        print(f\"Training time was {train_mins, train_secs}\")\n",
        "        print(\"run_id: {}; status: {}\".format(run.info.run_id, run.info.status))\n",
        "        print(\"--\")\n",
        "    \n",
        "        # Check for any active runs\n",
        "        print(\"Active run: {}\".format(mlflow.active_run()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2gGGYf-Fcsw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-0P6WB6nJ4Z"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgHJEmOu1u9K",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from pytorch_grad_cam import GradCAM\n",
        "\n",
        "# A model wrapper that gets a resnet model and returns the features before the fully connected layer.\n",
        "class ResnetFeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ResnetFeatureExtractor, self).__init__()\n",
        "        self.model = model\n",
        "        self.feature_extractor = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
        "                \n",
        "    def __call__(self, x):\n",
        "        return self.feature_extractor(x)[:, :, 0, 0]\n",
        "        \n",
        "resnet = torchvision.models.resnet50(pretrained=True)\n",
        "resnet.eval()\n",
        "feature_extract_model = ResnetFeatureExtractor(resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JArsA9Tzrrtz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0)\n",
        "def get_cosine_sim_score(feat_1, feat_2, cosine_fct=cos):\n",
        "    return (torch.sum(cos(feat_1.squeeze(), feat_2.squeeze())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGdBrHXUsKWd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "feature_extract_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a7k6A7O9z8W",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_bbox_coordinates_one_box(tensor): \n",
        "    all_x, all_y = (tensor.squeeze() == 1).nonzero(as_tuple=True)\n",
        "    smallest_x, smallest_y = torch.min(all_x).item(), torch.min(all_y).item()\n",
        "    largest_x, largest_y = torch.max(all_x).item(), torch.max(all_y).item()\n",
        "    return (smallest_y, smallest_x), (largest_y, largest_x)\n",
        "\n",
        "def get_foreground_background_embeddings(argmax_prediction_per_class, org_img, train_input, threshold, N_SEGMENTS, class_indx=1,  compactness=10, sigma=1, start_label=1, device=device, model=feature_extract_model):\n",
        "    # get superpixels\n",
        "    org_img = org_img.cpu().detach().numpy()\n",
        "    all_superpixels_mask = torch.from_numpy(slic(org_img, n_segments=N_SEGMENTS, compactness=compactness, sigma=sigma, start_label=start_label))\n",
        "    visualize_superpixels(all_superpixels_mask.numpy(), img=org_img)\n",
        "    hadamard = all_superpixels_mask.to(device) * argmax_prediction_per_class.to(device)\n",
        "    overlap = (hadamard / class_indx).type(torch.IntTensor)\n",
        "    # Instantiate base mask\n",
        "    base_mask = torch.zeros(overlap.shape)\n",
        "    # Get numbers to list, start from second element because first is 0 \n",
        "    relevant_superpixels = torch.unique(overlap).int().tolist()[1:]\n",
        "    relevant_superpixels_thresholded = []\n",
        "    for superpixel in relevant_superpixels:\n",
        "          temp = overlap.clone()\n",
        "          org = all_superpixels_mask.clone()\n",
        "        #   # Check how many are non-zero in superpixel mask\n",
        "          temp[temp != superpixel] = 0\n",
        "          org[org != superpixel] = 0\n",
        "          # Check how many are non-zero in overlap\n",
        "          # Determine share of pixels\n",
        "          share = torch.count_nonzero(temp).item() / torch.count_nonzero(org).item()\n",
        "          # Add superpixel as ones to base mask if share is over threshold\n",
        "          if share > threshold:\n",
        "            # bring org values to one\n",
        "            relevant_superpixels_thresholded.append(superpixel)\n",
        "    background_superpixels = [i.item() for i in torch.unique(all_superpixels_mask) if i not in relevant_superpixels_thresholded]\n",
        "    foreground_embeddings = torch.zeros([len(relevant_superpixels_thresholded), 2048])\n",
        "    background_embeddings = torch.zeros([len(background_superpixels), 2048])\n",
        "    for i, superpixel in enumerate(relevant_superpixels_thresholded):\n",
        "      all_superpixels_mask_tmp = all_superpixels_mask.clone()\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp != superpixel] = 0\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp>0] = 1\n",
        "      s,l = get_bbox_coordinates_one_box(all_superpixels_mask_tmp)\n",
        "      print(f\"Org shape: {torch.Tensor(org_img).permute(2,0,1).shape}\")\n",
        "      print(f\"Input shape: {train_input.shape}\")\n",
        "      base = torch.Tensor(org_img).permute(2,0,1).clone().cpu() / 255#train_input.clone().cpu()\n",
        "      base_aspm = base.clone()\n",
        "\n",
        "      base_aspm[0,:,:] = base_aspm[0,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[1,:,:] = base_aspm[1,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[2,:,:] = base_aspm[2,:,:] * all_superpixels_mask_tmp\n",
        "      cut = base_aspm[:,s[1]:l[1],s[0]:l[0]].unsqueeze(0).to(device)\n",
        "      visualize(cut=cut.cpu().squeeze(0).permute(2,1,0))\n",
        "      with torch.no_grad():\n",
        "        feat_foreground_sp = feature_extract_model(cut)\n",
        "        foreground_embeddings[i,:] = feat_foreground_sp\n",
        "    for i, superpixel in enumerate(background_superpixels):\n",
        "      all_superpixels_mask_tmp = all_superpixels_mask.clone()\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp != superpixel] = 0\n",
        "      all_superpixels_mask_tmp[all_superpixels_mask_tmp>0] = 1\n",
        "      s,l = get_bbox_coordinates_one_box(all_superpixels_mask_tmp)\n",
        "      base = train_input.clone().cpu()\n",
        "      base_aspm = base.clone()\n",
        "\n",
        "      base_aspm[0,:,:] = base_aspm[0,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[1,:,:] = base_aspm[1,:,:] * all_superpixels_mask_tmp\n",
        "      base_aspm[2,:,:] = base_aspm[2,:,:] * all_superpixels_mask_tmp\n",
        "      cut = base_aspm[:,s[1]:l[1],s[0]:l[0]].unsqueeze(0).to(device)\n",
        "      with torch.no_grad():\n",
        "        feat_background_sp = model(cut)\n",
        "        background_embeddings[i,:] = feat_background_sp\n",
        "    return foreground_embeddings, background_embeddings, relevant_superpixels_thresholded, all_superpixels_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvexgqX8F7fD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## Get mean embeddings\n",
        "\n",
        "# foreground_embeddings = torch.zeros([len(train_dataset), 2048])\n",
        "# backgound_embeddings = torch.zeros([len(train_dataset), 2048])\n",
        "# counter = 0\n",
        "# for epoch in range(0, 1):\n",
        "#   batch = 0\n",
        "#   with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "#     for train_inputs, train_labels, train_org_images in tepoch:\n",
        "#         batch += 1\n",
        "#         tepoch.set_description(f\"Epoch {epoch}\")\n",
        "#         train_inputs, train_labels, train_org_images = train_inputs.to(device), train_labels.to(device), train_org_images.to(device)\n",
        "#         for i in range(0,train_inputs.shape[0],1):\n",
        "#           i = 7\n",
        "#           N_SEGMENTS = 250\n",
        "#           threshold = 0.10\n",
        "#           vis = train_org_images[i].squeeze(0).permute(0,1,2).cpu()\n",
        "#           print(vis.shape)\n",
        "#           print(f\"train_input: {train_inputs.shape}\")\n",
        "#           print(f\"train_org_images: {train_org_images.shape}\")\n",
        "#           visualize(img=vis)\n",
        "#           embed_f, embed_b, rs, aspm = get_foreground_background_embeddings(train_labels[i], train_org_images[i], train_inputs[i], N_SEGMENTS=N_SEGMENTS, threshold=threshold)\n",
        "#           break\n",
        "#         break\n",
        "#   break\n",
        "          # mean_f = torch.mean(embed_f, dim=0)\n",
        "          # mean_b = torch.mean(embed_b, dim=0)\n",
        "\n",
        "          # foreground_embeddings[counter,:] += mean_f.cpu()\n",
        "          # backgound_embeddings[counter,:] += mean_b.cpu()\n",
        "          # counter += 1\n",
        "          # break\n",
        "          # if counter == 477:\n",
        "          #   print(\"if-break\")\n",
        "          #   # torch.save(foreground_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "          #   # torch.save(backgound_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "          #   break\n",
        "        # torch.save(foreground_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "        # torch.save(backgound_embeddings, 'PATH_TO_EMBEDDINGS')\n",
        "\n",
        "f = torch.load('PATH_TO_MEAN_EMBEDDING_VECTOR')\n",
        "b = torch.load('PATH_TO_MEAN_EMBEDDING_VECTOR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gngUDjKGTLw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def assign_foreground_sp(cosine_fct, mean_foreground_embedding, mean_background_embedding, relevant_superpixels_thresholded, foreground_embeddings, threshold):\n",
        "    close_f_foreground_embeddings = []\n",
        "    for i in range(foreground_embeddings.shape[0]):\n",
        "        f_cos = cosine_fct(mean_foreground_embedding, foreground_embeddings[i])\n",
        "        b_cos = cosine_fct(mean_background_embedding, foreground_embeddings[i])\n",
        "        diff = abs(b_cos - f_cos)\n",
        "        if (f_cos > b_cos or f_cos == b_cos) and diff <= threshold:\n",
        "          close_f_foreground_embeddings.append(relevant_superpixels_thresholded[i])\n",
        "    return close_f_foreground_embeddings\n",
        "\n",
        "def scan_outer_boundary(superpixel_mask, flatten_list_fct=flatten):\n",
        "    # We will collect those superpixels which are on the outer boundary so they \n",
        "    # have a zero neighbour.\n",
        "    superpixel_mask_refined = superpixel_mask.clone()\n",
        "    outer_superpixel_rows, outer_superpixel_cols, outer_all = [], [], []\n",
        "    tuples = torch.nonzero(superpixel_mask_refined)\n",
        "    rows = torch.unique(tuples[:,0])\n",
        "    columns = torch.unique(tuples[:,1])\n",
        "    # scan over rows \n",
        "    for i in rows:\n",
        "      current_row = superpixel_mask_refined[i, :]\n",
        "      unique_non_zeroed_row = torch.unique(current_row[current_row.nonzero(as_tuple=True)], sorted=False)\n",
        "      first_superpixel = unique_non_zeroed_row[-1]\n",
        "      last_superpixel = unique_non_zeroed_row[0]\n",
        "      outer_superpixel_rows.append(first_superpixel.item())\n",
        "      outer_superpixel_rows.append(last_superpixel.item())\n",
        "    # scan over columns\n",
        "    for i in columns:\n",
        "      current_column = superpixel_mask_refined[:, i]\n",
        "      unique_non_zeroed_column = torch.unique(current_column[current_column.nonzero(as_tuple=True)], sorted=False)\n",
        "      first_superpixel = unique_non_zeroed_column[-1]\n",
        "      last_superpixel = unique_non_zeroed_column[0]\n",
        "      outer_superpixel_cols.append(first_superpixel.item())\n",
        "      outer_superpixel_cols.append(last_superpixel.item())\n",
        "    outer_all.append(outer_superpixel_cols)\n",
        "    outer_all. append(outer_superpixel_rows)\n",
        "    outer_all = flatten(outer_all)\n",
        "    return list(set(outer_all))\n",
        "\n",
        "def create_embedding_mask(train_label, \n",
        "                          train_org_image, \n",
        "                          train_input, \n",
        "                          N_SEGMENTS, \n",
        "                          threshold_embedding=0, \n",
        "                          threshold_closeness=0, \n",
        "                          mean_foreground_embedding=mean_f, \n",
        "                          mean_background_embedding=mean_b, \n",
        "                          cosine_function=get_cosine_sim_score, \n",
        "                          assign_label_based_on_closeness=assign_foreground_sp, \n",
        "                          get_foreground_background_embeddings_function=get_foreground_background_embeddings, \n",
        "                          scan_outer_pixels=True,\n",
        "                          scan_outer_superpixels_function=scan_outer_boundary,\n",
        "                          postprocess_crf=True,\n",
        "                          iter=1):\n",
        "    foreground_embeddings, background_embeddings, relevant_superpixels, all_superpixels_mask = get_foreground_background_embeddings_function(train_label, train_org_image, train_input, N_SEGMENTS=N_SEGMENTS, threshold=threshold_embedding)\n",
        "    for i in range(0, iter, 1):\n",
        "      not_in_relevant_superpixels = [i for i in torch.unique(all_superpixels_mask) if i not in relevant_superpixels]\n",
        "      embedding_mask_relevant_superpixels = all_superpixels_mask.clone()\n",
        "      for i in not_in_relevant_superpixels:\n",
        "          embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels == i] = 0\n",
        "      if scan_outer_pixels == True:\n",
        "          outer_superpixels = scan_outer_superpixels_function(embedding_mask_relevant_superpixels)\n",
        "          indexes_outer = [relevant_superpixels.index(i) for i in outer_superpixels]\n",
        "          outer_foreground_embedding = foreground_embeddings[indexes_outer, :]\n",
        "          close_foreground_outer_superpixels = assign_foreground_sp(cosine_function, mean_foreground_embedding, mean_background_embedding, relevant_superpixels_thresholded=outer_superpixels, foreground_embeddings=outer_foreground_embedding, threshold=threshold_closeness)\n",
        "          to_be_dropped = [i for i in outer_superpixels if i not in close_foreground_outer_superpixels]\n",
        "          relevant_superpixels = [i for i in relevant_superpixels if i not in to_be_dropped]\n",
        "          not_in_relevant_superpixels = [i for i in torch.unique(all_superpixels_mask) if i not in relevant_superpixels]\n",
        "          for i in not_in_relevant_superpixels:\n",
        "              embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels == i] = 0\n",
        "    embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels > 0] = 1\n",
        "    if postprocess_crf == True:\n",
        "      embedding_mask_relevant_superpixels = crf(train_org_image, embedding_mask_relevant_superpixels)\n",
        "    else:\n",
        "      embedding_mask_relevant_superpixels[embedding_mask_relevant_superpixels > 0] = 1\n",
        "      if postprocess_crf == True:\n",
        "          embedding_mask_relevant_superpixels = crf(train_org_image, embedding_mask_relevant_superpixels)\n",
        "\n",
        "    return embedding_mask_relevant_superpixels, outer_superpixels, embedding_mask_relevant_superpixels, all_superpixels_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM34i05halkn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def export_superpixel_embedding_masks(dataset, export_path=\"PATH\", img_transform=img_transform, show_grad_cam_on_img=show_grad_cam_on_img):\n",
        "      images = dataset.X\n",
        "      masks = dataset.Y\n",
        "      for i, _ in tqdm(enumerate(images)):\n",
        "          # load image\n",
        "          img = torch.tensor(imread(_))\n",
        "          # load mask\n",
        "          if \".tif\" in masks[i]:\n",
        "            mask = torch.tensor(imread(masks[i])).long()\n",
        "          elif \".png\" in masks[i]:\n",
        "            mask = torch.Tensor(np.array(Image.open(masks[i]))).long()\n",
        "          mask[mask>0] = 1    \n",
        "          embedding_mask_outer_fct = create_embedding_mask(mask, \n",
        "                                                     img, \n",
        "                                                     img_transform(imread(_)), \n",
        "                                                     N_SEGMENTS=200, \n",
        "                                                     iter=1)\n",
        "          embedding_mask_outer_fct = pass_pseudomask_or_ground_truth(mask.to(device), embedding_mask_outer_fct.to(device), iou_threshold=0.5, mask_occupancy_threshold=0.05)\n",
        "          embedding_mask_outer_fct = pass_pseudomask_or_ground_truth(mask.to(device), embedding_mask_outer_fct)\n",
        "          overlay_1 = show_grad_cam_on_img(img, embedding_mask_outer_fct.cpu())\n",
        "          visualize(img=img.cpu(), gt_box=mask.cpu(), embedding_mask_outer_fct=embedding_mask_outer_fct.cpu(), overlay_1=overlay_1)  \n",
        "          embedding_mask_outer_fct = Image.fromarray(np.uint8(embedding_mask_outer_fct.cpu().detach() * 255) , 'L')\n",
        "          output_path_mask = (\n",
        "          export_path + \"/\" + _.split('/')[-1]\n",
        "          ).replace(\"tif\", \"png\")\n",
        "          embedding_mask_outer_fct.save(output_path_mask, quality=100, subsampling=0)\n",
        "# export_superpixel_embedding_masks(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8Qmak0Op98Cb",
        "g-0P6WB6nJ4Z"
      ],
      "name": "colonoscopy_paper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "exp",
      "language": "python",
      "name": "exp"
    },
    "vscode": {
      "interpreter": {
        "hash": "3e03cce523887ef4ae3cc71c66ab4ee9a46035781549cd05776eda36e5299632"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
